# ディープラーニングにおける汎化

:numref:`chap_regression`と:numref:`chap_classification`では、線形モデルをトレーニングデータに適合させることにより、回帰と分類の問題に取り組みました。どちらの場合も、観測されたトレーニングラベルの可能性を最大化するパラメーターを見つけるための実用的なアルゴリズムを提供しました。そして、各章の終わりに向かって、トレーニングデータのフィッティングは中間的な目標にすぎないことを思い出しました。私たちの本当の探求は、同じ母集団から引き出された新しい例でも正確な予測を行うことができる*一般的なパターン*を発見することでした。機械学習の研究者は、最適化アルゴリズムの「消費者」です。時には、新しい最適化アルゴリズムを開発しなければならないこともあります。しかし、結局のところ、最適化は単に目的を達成するための手段にすぎません。その核となるのは、機械学習は統計的な分野であり、何らかの統計的原理（既知または未知）によって結果として得られるモデルがトレーニングセットを超えて一般化される場合に限り、トレーニング損失を最適化したいと考えています。 

明るい面として、確率的勾配降下法によって訓練されたディープニューラルネットワークは、コンピュータービジョン、自然言語処理、時系列データ、レコメンダーシステム、電子健康記録、タンパク質の折りたたみ、値関数にまたがる無数の予測問題にわたって非常にうまく一般化することがわかります。ビデオゲームやボードゲームの近似; そして無数の他のドメイン。欠点として、最適化ストーリー（トレーニングデータに適合させることができる理由）またはジェネラライズストーリー（結果として得られるモデルが目に見えない例に一般化される理由）のどちらかを簡単に説明したい場合は、自分で飲み物を注ぐことをお勧めします。線形モデルを最適化する手順とソリューションの統計的特性は、どちらも包括的な理論体系によって十分に説明されていますが、ディープラーニングの理解は、両面で依然として西部開拓時代に似ています。 

ディープラーニングの理論と実践は両方の面で急速に進化しており、理論家は何が起こっているのかを説明する新しい戦略を採用しています。実践者が猛烈なペースで革新を続けているにもかかわらず、深いネットワークと直感と民俗知識を訓練するためのヒューリスティックの武器を構築しています。どのテクニックをどの状況に適用するかを決定するためのガイダンスを提供します。 

現在のTL; DRは、ディープラーニングの理論が有望な攻撃ラインを生み出し、魅力的な結果を散在させているということですが、（i）ニューラルネットワークを最適化できる理由と（ii）勾配降下法によって学習されたモデルがどのように管理できるかの両方の包括的な説明からはほど遠いようです高次元のタスクでもうまく一般化してください。しかし、実際には、(i) が問題になることはほとんどありません (すべてのトレーニングデータに適合するパラメータを常に見つけることができます)。したがって、汎化を理解することははるかに大きな問題です。一方、首尾一貫した科学理論の快適さがなくても、実務家は、実際に一般化するモデルを作成するのに役立つ可能性のある多数の技術を開発しました。簡潔な要約は、ディープラーニングの一般化という広大なトピックを正当化することはできず、研究の全体的な状態は解決にはほど遠いものの、このセクションでは、研究と実践の現状の広い概要を提示することを願っています。 

## オーバーフィットと正則化を再考する

機械学習モデルのトレーニングに対する私たちのアプローチは、通常、（i）トレーニングデータを適合させること、および（ii）ホールドアウトデータでモデルを評価することによって*汎化誤差*（基礎となる母集団の真の誤差）を推定する2つのフェーズで構成されることを思い出してください。トレーニングデータへの適合とテストデータへの適合度の差は*汎化ギャップ*と呼ばれ、汎化ギャップが大きい場合、モデルはトレーニングデータに*オーバーフィット*します。過適合の極端なケースでは、テスト誤差が有意なままであっても、トレーニングデータを正確に近似する可能性があります。そして、古典的な見方では、私たちのモデルは複雑すぎると解釈され、特徴の数、学習された非ゼロのパラメーターの数、または定量化されたパラメーターのサイズを縮小する必要があります。:numref:`sec_generalization_basics`のモデルの複雑度対損失（:numref:`fig_capacity_vs_error`）のプロットを思い出してください。 

しかし、ディープラーニングはこの状況を直観に反する形で複雑にします。まず、分類問題の場合、私たちのモデルは通常、数百万の:cite:`zhang2021understanding`で構成されるデータセットであっても、すべてのトレーニング例に完全に適合するのに十分な表現力があります。古典的な図では、この設定はモデルの複雑度軸の右端にあり、汎化誤差の改善は、モデルクラスの複雑さを軽減するか、またはペナルティを適用してセットを厳しく制約することによって、正則化によってもたらされなければならないと考えるかもしれません。私たちのパラメータが取るかもしれない値の。しかし、それは物事が奇妙になり始めるところです。 

奇妙なことに、多くのディープラーニングタスク（画像認識やテキスト分類など）では、通常、モデルアーキテクチャの中から選択しています。これらのアーキテクチャはすべて、任意に低いトレーニング損失（およびトレーニングエラーゼロ）を達成できます。検討中のすべてのモデルがゼロトレーニングエラーを達成するため、
*さらなる利益を得る唯一の手段は、オーバーフィッティングを減らすことです*。
さらに奇妙なことに、トレーニングデータを完全に適合させても、レイヤーやノードを追加したり、より多くのエポックでトレーニングしたりするなど、モデルを*より表現力豊かに*することで、実際にジェネラライズエラーをさらに削減*できることがよくあります。しかし、奇妙なことに、ジェネラライズギャップをモデルの*複雑さ*に関連付けるパターン（ネットワークの深さや幅などでキャプチャされたもの）は単調ではなく、最初は複雑さが大きくなりますが、その後、いわゆる「二重降下」パターンに役立ちます。:cite:`nakkiran2021deep`。したがって、ディープラーニングの実践者は、ある意味でモデルを制限しているように見えるものと、モデルをさらに表現力豊かにすると思われるものと、ある意味で過適合を緩和するためにすべて適用されるトリックの袋を持っています。 

さらに複雑なことに、古典的学習理論によって提供される保証は古典モデルであっても保守的である可能性がありますが、そもそもディープニューラルネットワークが一般化される理由を説明するには無力に見えます。ディープニューラルネットワークは、大規模なデータセットに対しても任意のラベルを当てはめることができるため、$\ell_2$正則化のような使い慣れた方法を使用しているにもかかわらず、従来の複雑さに基づく汎化限界（仮説クラスのVC次元またはRademacher複雑度に基づくものなど）では、ニューラルネットワークが一般化する理由を説明する。 

## ノンパラメトリクスからのインスピレーション

ディープラーニングに初めて近づくと、それらをパラメトリックモデルと考えるのは魅力的です。結局のところ、モデルには何百万ものパラメータがあります。モデルを更新すると、そのパラメータが更新されます。モデルを保存すると、パラメータがディスクに書き込まれます。しかしながら, 数学とコンピューターサイエンスは、直感に反する視点の変化に満ちています, 驚くべき同型は一見異なる問題.ニューラルネットワークは明らかにパラメータを「持っている」ものの、ある意味では、ノンパラメトリックモデルのように振る舞うと考える方が実り多いかもしれません。では、モデルをノンパラメトリックにする正確な理由は何ですか？この名前にはさまざまなアプローチが含まれていますが、共通のテーマの1つは、ノンパラメトリック手法は、利用可能なデータ量が増えるにつれて複雑になる傾向があるということです。 

おそらく、ノンパラメトリックモデルの最も単純な例は、$k$-最近傍アルゴリズムです（:numref:`sec_attention-pooling`など、より多くのノンパラメトリックモデルについては後で説明します）。ここで、学習者は学習時に、データセットを単に記憶します。次に、予測時に新しい点 $\mathbf{x}$ が検出されると、学習者は $k$ 最近傍を調べます ($k$ ポイント $\mathbf{x}_i'$ はある程度の距離を最小化します $d(\mathbf{x}, \mathbf{x}_i')$)。$k=1$ の場合、このアルゴリズムは 1 最近傍と呼ばれ、アルゴリズムは常にゼロの学習誤差を達成します。しかし、それはアルゴリズムが一般化されないという意味ではありません。実際、ある穏やかな条件下では、1最近傍アルゴリズムは一貫している（最終的には最適な予測変数に収束する）ことが分かります。 

1つの最近傍では、何らかの距離関数$d$を指定するか、またはそれと同等に、データを特徴付けるために何らかのベクトル値基底関数$\phi(\mathbf{x})$を指定する必要があることに注意してください。どの距離計量を選択しても、学習誤差が0になり、最終的には最適な予測変数に到達しますが、距離計量 $d$ によってさまざまな誘導バイアスがエンコードされ、利用可能なデータ量が限られていると、異なる予測変数が生成されます。距離計量 $d$ のさまざまな選択肢は、基礎となるパターンに関するさまざまな仮定を表し、さまざまな予測変数の性能は、仮定と観測データとの適合性によって異なります。 

ある意味では、ニューラルネットワークはオーバーパラメーター化されており、トレーニングデータの近似に必要以上のパラメーターがあるため、トレーニングデータを「補間」する（完全に適合する）傾向があるため、ある意味ではノンパラメトリックモデルのように動作します。最近の理論的研究により、大規模ニューラルネットワークとノンパラメトリック手法、特にカーネル法との間に深いつながりが確立されています。特に、:cite:`Jacot.Grabriel.Hongler.2018`は、限界内で、ランダムに初期化された重みを持つ多層パーセプトロンが無限に広くなるにつれて、ニューラルタンジェントと呼ばれるカーネル関数（本質的には距離関数）の特定の選択に対する（ノンパラメトリック）カーネル法と同等になることを実証しました。カーネル。現在のニューラルタンジェントカーネルモデルは、最新のディープネットワークの動作を完全には説明できないかもしれませんが、解析ツールとしての成功は、オーバーパラメーター化されたディープネットワークの動作を理解するためのノンパラメトリックモデリングの有用性を強調しています。 

## 早期停止

ディープニューラルネットワークは任意のラベルを当てはめることができますが、ラベルが誤ってまたはランダムに割り当てられた場合でも :cite:`zhang2021understanding`、この能力はトレーニングの反復を何度も繰り返すことによってのみ現れます。新しい作業ライン:cite:`Rolnick.Veit.Belongie.Shavit.2017`は、ラベルノイズの設定において、ニューラルネットワークがきれいにラベル付けされたデータを最初に適合させ、その後にのみ誤ったラベル付けされたデータを補間する傾向があることを明らかにしました。さらに、この現象は一般化の保証に直接変換されることが確立されています。モデルがトレーニングセットに含まれるランダムにラベル付けされた例ではなく、きれいにラベル付けされたデータに適合する場合は常に、実際には:cite:`Garg.Balakrishnan.Kolter.Lipton.2021`を一般化しています。 

これらの知見を合わせると、ディープニューラルネットワークを正則化するための古典的な手法である*早期停止*の動機付けに役立ちます。ここでは、重みの値を直接制約するのではなく、トレーニングのエポック数を制約します。停止基準を決定する最も一般的な方法は、トレーニング全体で検証エラーを監視し（通常、各エポックの後に1回チェックする）、検証エラーがいくつかのエポックで少しだけ減少していないときにトレーニングを遮断することです。$\epsilon$。これは*忍耐基準*と呼ばれることもあります。より一般化につながる可能性に加えて、ノイズの多いラベルの設定では、早期停止のもう1つの利点は時間の節約です。忍耐の基準が満たされると、トレーニングを終了できます。8つ以上のGPUで同時に数日間のトレーニングを必要とする大規模なモデルの場合、適切に調整された早期停止により、研究者の時間を節約し、雇用者を何千ドルも節約できます。 

特に、ラベルノイズがなく、データセットが*実現可能*である場合（クラスが本当に分離可能、たとえば猫と犬を区別するなど）、早期停止は一般化の大幅な改善につながらない傾向があります。一方、ラベルノイズやラベルに固有のばらつきがある場合（患者の死亡率を予測するなど）、早期停止が重要です。ノイズの多いデータを内挿するまでモデルをトレーニングすることは、一般的に悪い考えです。 

## ディープネットワークのための古典的正則化手法

:numref:`chap_regression`では、モデルの複雑さを制約するためのいくつかの古典的な正則化手法について説明しました。特に、:numref:`sec_weight_decay`は、重み減衰と呼ばれる方法を導入しました。これは、損失関数に正則化項を追加して、大きな値の重みにペナルティを課すことで構成されます。どのウェイトノルムにペナルティが課されるかに応じて、この手法はリッジ正則化 ($\ell_2$ ペナルティ) またはラッソ正則化 ($\ell_1$ ペナルティの場合) として知られています。これらの正則化器の古典的解析では、モデルが任意のラベルに当てはまるのを防ぐために、重みが取ることができる値を制限すると考えられています。 

ディープラーニングの実装では、体重減衰が依然として一般的なツールです。しかし、研究者は、$\ell_2$正則化の典型的な強みは、ネットワークがデータ:cite:`zhang2021understanding`を補間するのを防ぐには不十分であり、したがって、正則化として解釈された場合の利点は、早期停止基準と組み合わせてのみ意味をなす可能性があることを指摘しています。早期停止がない場合、層数やノード数（ディープラーニングの場合）または距離計量（1最近傍内）と同様に、これらの方法はニューラルネットワークのパワーを有意義に制約するためではなく、何らかの形でより良い一般化につながる可能性があります。関心のあるデータセットで見つかったパターンとの互換性が高い誘導バイアスをエンコードします。したがって、古典的正則化器は、その有効性の理論的根拠が根本的に異なっていても、ディープラーニングの実装では依然として人気があります。 

特に、ディープラーニングの研究者は、モデル入力にノイズを追加するなど、古典的な正則化のコンテキストで最初に普及した手法も構築しています。次のセクションでは、ディープラーニングの有効性の理論的根拠が同様に謎のままであるにもかかわらず、ディープラーニングの主力となった有名なドロップアウト手法（:citet:`Srivastava.Hinton.Krizhevsky.ea.2014`によって発明された）を紹介します。 

## まとめ

例よりもパラメーターが少ない傾向がある古典的な線形モデルとは異なり、ディープネットワークはパラメーター化しすぎる傾向があり、ほとんどのタスクでトレーニングセットを完全に適合させることができます。この*補間レジーム*は、多くの難しい素早い直感に挑戦します。機能的には、ニューラルネットワークはパラメトリックモデルのように見えます。しかし、それらをノンパラメトリックモデルと考えることは、直感のより信頼できる情報源になる場合があります。検討中のすべてのディープネットワークがすべての学習ラベルを近似できることはよくあることなので、ほぼすべての利益は過適合を緩和する（*汎化のギャップ*を埋める）ことによって得られなければなりません。逆説的に、汎化ギャップを減少させる介入は、モデルの複雑さを増すように見える場合や、複雑さを軽減するように見える場合があります。しかし、これらの方法は、古典理論がディープネットワークの一般化を説明するのに十分なほど複雑さを低下させることはめったになく、*特定の選択が一般化の改善につながる理由*は、多くの優秀な研究者の協調的な努力にもかかわらず、大部分が未解決の問題のままです。 

## 演習

1. 従来の複雑性に基づく測定では、ディープニューラルネットワークの一般化を説明できないのはどのような意味ですか？
1. なぜ*早期停止*が正則化手法と見なされるのでしょうか？
1. 研究者は通常、停止基準をどのように決定しますか？
1. 早期停止が一般化の大幅な改善につながるケースを区別する重要な要素は何ですか？
1. 一般化を超えて、早期停止の別の利点を説明する。

[Discussions](https://discuss.d2l.ai/t/7473)
