```{.python .input}
%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow'])
```

# 数値の安定性と初期化
:label:`sec_numerical_stability`

これまで、実装したすべてのモデルでは、事前に指定された分布に従ってパラメータを初期化する必要がありました。これまで、私たちは初期化スキームを当然のことと思い、これらの選択がどのように行われるかの詳細を詳しく説明しました。これらの選択は特に重要ではないという印象を受けたかもしれません。逆に、初期化スキームの選択はニューラルネットワーク学習において重要な役割を果たし、数値の安定性を維持するために重要になる可能性があります。さらに、これらの選択肢は、非線形活性化関数の選択と興味深い方法で結びつけることができます。どの関数を選択し、どのようにパラメータを初期化するかによって、最適化アルゴリズムの収束速度が決まります。ここでの選択が悪いと、トレーニング中にグラデーションが爆発したり消失したりすることがあります。このセクションでは、これらのトピックをより詳細に掘り下げ、ディープラーニングのキャリアを通じて役立ついくつかの有用なヒューリスティックについて説明します。 

## グラデーションの消失と爆発

$L$ 層、入力 $\mathbf{x}$、出力 $\mathbf{o}$ を持つディープネットワークを考えてみましょう。各レイヤー$l$は、重み$\mathbf{W}^{(l)}$でパラメーター化された変換$f_l$によって定義され、その隠れレイヤー出力は$\mathbf{h}^{(l)}$（$\mathbf{h}^{(0)} = \mathbf{x}$としましょう）であるため、ネットワークは次のように表すことができます。 

$$\mathbf{h}^{(l)} = f_l (\mathbf{h}^{(l-1)}) \text{ and thus } \mathbf{o} = f_L \circ \ldots \circ f_1(\mathbf{x}).$$

すべての隠れ層の出力と入力がベクトルである場合、$\mathbf{o}$の任意のパラメータセットに対する$\mathbf{o}$の勾配を次のように記述できます。 

$$\partial_{\mathbf{W}^{(l)}} \mathbf{o} = \underbrace{\partial_{\mathbf{h}^{(L-1)}} \mathbf{h}^{(L)}}_{ \mathbf{M}^{(L)} \stackrel{\mathrm{def}}{=}} \cdot \ldots \cdot \underbrace{\partial_{\mathbf{h}^{(l)}} \mathbf{h}^{(l+1)}}_{ \mathbf{M}^{(l+1)} \stackrel{\mathrm{def}}{=}} \underbrace{\partial_{\mathbf{W}^{(l)}} \mathbf{h}^{(l)}}_{ \mathbf{v}^{(l)} \stackrel{\mathrm{def}}{=}}.$$

つまり、この勾配は $L-l$ 行列 $\mathbf{M}^{(L)} \cdot \ldots \cdot \mathbf{M}^{(l+1)}$ と勾配ベクトル $\mathbf{v}^{(l)}$ の積です。したがって、あまりにも多くの確率を掛け合わせるとしばしば発生する数値アンダーフローの同じ問題の影響を受けやすくなります。確率を扱う場合、一般的なトリックは対数空間に切り替えることです。つまり、圧力を仮数部から数値表現の指数に移すことです。残念ながら、上記の問題はより深刻です。最初に行列 $\mathbf{M}^{(l)}$ はさまざまな固有値を持つ可能性があります。それらは小さい場合も大きい場合もあれば、製品が*非常に大きい*または*非常に小さい*場合もあります。 

不安定な勾配によってもたらされるリスクは、数値表現を超えています。予測不可能な大きさの勾配も、最適化アルゴリズムの安定性を脅かします。(i) 過度に大きく、モデルを破壊する (*爆発する勾配*問題)、(ii) 過度に小さい (*消失する勾配*の問題)、更新のたびにパラメータがほとんど移動しないため、学習が不可能になるパラメータの更新に直面している可能性があります。 

### (**消えるグラデーション**)

消失勾配の問題を引き起こす原因の 1 つは、各層の線形演算の後に追加される活性化関数 $\sigma$ の選択です。歴史的に、シグモイド関数 $1/(1 + \exp(-x))$ (:numref:`sec_mlp` で導入) は、しきい値関数に似ているため人気がありました。初期の人工ニューラルネットワークは生物学的ニューラルネットワークに触発されていたため、（生物学的ニューロンのように）*完全に*または*まったく*発火しないニューロンのアイデアは魅力的に思えました。シグモイドを詳しく見て、勾配の消失を引き起こす原因を見てみましょう。

```{.python .input}
%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
npx.set_np()

x = np.arange(-8.0, 8.0, 0.1)
x.attach_grad()
with autograd.record():
    y = npx.sigmoid(x)
y.backward()

d2l.plot(x, [y, x.grad], legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))
```

```{.python .input}
%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch

x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.sigmoid(x)
y.backward(torch.ones_like(x))

d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()],
         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))
```

```{.python .input}
%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf

x = tf.Variable(tf.range(-8.0, 8.0, 0.1))
with tf.GradientTape() as t:
    y = tf.nn.sigmoid(x)
d2l.plot(x.numpy(), [y.numpy(), t.gradient(y, x).numpy()],
         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))
```

ご覧のとおり、(**シグモイドの勾配は、入力が大きい場合と小さい場合の両方で消失します**)。さらに、多くの層を逆伝播する場合、多くのシグモイドへの入力がゼロに近いゴルディロックスゾーンにいない限り、製品全体の勾配が消える可能性があります。私たちのネットワークが多くのレイヤーを誇っている場合、注意しない限り、グラデーションはあるレイヤーで切り取られる可能性があります。実際、この問題はかつてディープネットワークトレーニングを悩ませていました。その結果、より安定した（しかし神経的にもっともらしくない）ReLUは、開業医のデフォルトの選択肢として浮上しています。 

### [**グラデーションの展開**]

グラデーションが爆発するときの反対の問題は、同様に厄介です。これをもう少しわかりやすく説明するために、100個のガウスランダム行列を描き、それらに初期行列を掛けます。選択した尺度（分散$\sigma^2=1$の選択）では、行列積が爆発的に増加します。ディープネットワークの初期化によってこれが発生した場合、勾配降下オプティマイザが収束する可能性はありません。

```{.python .input}
%%tab mxnet
M = np.random.normal(size=(4, 4))
print('a single matrix', M)
for i in range(100):
    M = np.dot(M, np.random.normal(size=(4, 4)))

print('after multiplying 100 matrices', M)
```

```{.python .input}
%%tab pytorch
M = torch.normal(0, 1, size=(4, 4))
print('a single matrix \n',M)
for i in range(100):
    M = M @ torch.normal(0, 1, size=(4, 4))

print('after multiplying 100 matrices\n', M)
```

```{.python .input}
%%tab tensorflow
M = tf.random.normal((4, 4))
print('a single matrix \n', M)
for i in range(100):
    M = tf.matmul(M, tf.random.normal((4, 4)))

print('after multiplying 100 matrices\n', M.numpy())
```

### シンメトリーを破る

ニューラルネットワーク設計におけるもう一つの問題は、それらのパラメータ化に内在する対称性です。1 つの隠れ層と 2 つのユニットを持つ単純な MLP があると仮定します。この場合、最初の層の重み $\mathbf{W}^{(1)}$ を順列化し、同様に出力層の重みを置換して同じ関数を得ることができます。最初の隠しユニットと2番目の隠しユニットを区別する特別なことは何もありません。言い換えれば、各レイヤーの隠れたユニット間に順列対称性があります。 

これは単なる理論上の迷惑ではありません。前述の 2 つの隠れユニットを持つ 1 つの隠れ層 MLP を考えてみましょう。説明のために、出力レイヤーが 2 つの非表示ユニットを 1 つの出力ユニットだけに変換するとします。いくつかの定数$c$に対して、隠れ層のすべてのパラメータを$\mathbf{W}^{(1)} = c$として初期化したらどうなるか想像してみてください。この場合、順伝播中、いずれかの隠しユニットが同じ入力とパラメータを受け取り、同じアクティベーションを生成し、出力ユニットに供給されます。バックプロパゲーション中、パラメーター $\mathbf{W}^{(1)}$ に関して出力単位を微分すると、要素がすべて同じ値を取る勾配が得られます。したがって、勾配ベースの反復 (ミニバッチ確率的勾配降下法など) の後でも、$\mathbf{W}^{(1)}$ のすべての要素は同じ値をとります。このような反復は、それ自体で「対称性を破る」ことはなく、ネットワークの表現力を実現することは決してできないかもしれません。非表示のレイヤーは、あたかもユニットが 1 つしかないかのように動作します。ミニバッチの確率的勾配降下法はこの対称性を壊さないが、ドロップアウト正則化（後で紹介する）はそうなることに注意してください！ 

## パラメーターの初期化

上記で提起された問題に対処する、または少なくとも軽減する1つの方法は、慎重に初期化することです。後で説明するように、最適化中の追加の注意と適切な正則化により、安定性をさらに高めることができます。 

### 既定の初期化

前のセクション、たとえば:numref:`sec_linear_concise`では、正規分布を使用して重みの値を初期化しました。初期化方法を指定しない場合、フレームワークはデフォルトのランダム初期化方法を使用します。これは、中程度の問題サイズに対して実際にうまく機能することがよくあります。 

### ザビエル初期化
:label:`subsec_xavier`

全結合層の出力 $o_{i}$ のスケール分布を見てみましょう。
*非線形性なし*。
$n_\mathrm{in}$ 入力 $x_j$ と、この層に関連する重み $w_{ij}$ の場合、出力は次の式で与えられます。 

$$o_{i} = \sum_{j=1}^{n_\mathrm{in}} w_{ij} x_j.$$

重み $w_{ij}$ はすべて同じ分布から独立して描画されます。さらに、この分布にはゼロ平均と分散$\sigma^2$があると仮定します。これは、分布がガウス分布でなければならないという意味ではなく、平均と分散が存在する必要があるということだけを意味することに注意してください。今のところ、レイヤー $x_j$ への入力もゼロの平均と分散 $\gamma^2$ をもち、$w_{ij}$ から独立していて互いに独立していると仮定します。この場合、$o_i$の平均と分散は次のように計算できます。 

$$
\begin{aligned}
    E[o_i] & = \sum_{j=1}^{n_\mathrm{in}} E[w_{ij} x_j] \\&= \sum_{j=1}^{n_\mathrm{in}} E[w_{ij}] E[x_j] \\&= 0, \\
    \mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\
        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\
        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\
        & = n_\mathrm{in} \sigma^2 \gamma^2.
\end{aligned}
$$

分散を固定する 1 つの方法は、$n_\mathrm{in} \sigma^2 = 1$ を設定することです。ここで、バックプロパゲーションについて考えてみましょうそこでは、出力に近い層から勾配が伝播されるにもかかわらず、同様の問題に直面します。順伝播と同じ推論を使用して、$n_\mathrm{out} \sigma^2 = 1$（$n_\mathrm{out}$）がこの層の出力数でない限り、勾配の分散が爆発する可能性があることがわかります。これにより、私たちはジレンマに陥ります。両方の条件を同時に満たすことはできません。代わりに、私たちは単に以下を満足させようとします。 

$$
\begin{aligned}
\frac{1}{2} (n_\mathrm{in} + n_\mathrm{out}) \sigma^2 = 1 \text{ or equivalently }
\sigma = \sqrt{\frac{2}{n_\mathrm{in} + n_\mathrm{out}}}.
\end{aligned}
$$

これが、その作成者:cite:`Glorot.Bengio.2010`の最初の作者にちなんで名付けられた、現在標準的で実用的に有益な*Xavier初期化*の根底にある理由です。通常、Xavier の初期化は、ゼロ平均、分散 $\sigma^2 = \frac{2}{n_\mathrm{in} + n_\mathrm{out}}$ をもつガウス分布から重みをサンプリングします。また、ザビエルの直感を応用して、一様分布から重みをサンプリングするときの分散を選択することもできます。一様分布$U(-a, a)$には分散$\frac{a^2}{3}$があることに注意してください。$\frac{a^2}{3}$を$\sigma^2$の条件に差し込むと、それに従って初期化する提案が得られます。 

$$U\left(-\sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}, \sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}\right).$$

上記の数学的推論における非線形性の非存在の仮定は、ニューラルネットワークでは簡単に破られる可能性がありますが、ザビエルの初期化方法は実際にはうまく機能することがわかりました。 

### 超えて

上記の推論は、パラメータの初期化に対する最新のアプローチの表面をほとんど傷つけません。ディープラーニングフレームワークは、多くの場合、十数種類以上のヒューリスティックを実装します。さらに、パラメーターの初期化は、ディープラーニングの基礎研究の注目の分野であり続けています。これらの中には、関連付けられた（共有された）パラメータ、超解像度、シーケンスモデル、およびその他の状況に特化したヒューリスティックがあります。たとえば、Xiaoらは、慎重に設計された初期化方法:cite:`Xiao.Bahri.Sohl-Dickstein.ea.2018`を使用して、アーキテクチャ上のトリックなしで10000層のニューラルネットワークをトレーニングする可能性を示しました。 

トピックに興味がある場合は、このモジュールの提供内容を深く掘り下げ、各ヒューリスティックを提案および分析した論文を読み、そのトピックに関する最新の出版物を調べることをお勧めします。おそらく、あなたはつまずいたり、巧妙なアイデアを発明したり、ディープラーニングフレームワークの実装に貢献したりするでしょう。 

## まとめ

* 深層ネットワークでは、勾配の消失と爆発が一般的な問題です。勾配とパラメータを適切に制御するには、パラメータの初期化には細心の注意が必要です。
* 初期勾配が大きすぎたり小さすぎたりしないようにするには、初期化ヒューリスティックが必要です。
* ReLU 活性化関数は消失勾配の問題を軽減します。これにより、コンバージェンスが加速されます。
* ランダム初期化は、最適化の前に対称性が破られるようにするための鍵です。
* ザビエルの初期化では、各層について、出力の分散は入力数の影響を受けず、勾配の分散は出力の数に影響されないことが示唆されています。

## 演習

1. MLPのレイヤーの順列対称性以外に、ニューラルネットワークが破壊を必要とする対称性を示す可能性がある他のケースを設計できますか？
1. 線形回帰またはソフトマックス回帰のすべての重みパラメータを同じ値に初期化できますか？
1. 2 つの行列の積の固有値の解析的限界を調べます。これは、グラデーションが適切に調整されていることを確認することについて何を教えてくれますか？
1. いくつかの用語が分かれていることがわかっている場合、事後にこれを修正できますか？インスピレーションを得るために、レイヤーごとの適応レートスケーリングに関する論文を見てください :cite:`You.Gitman.Ginsburg.2017`。

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/103)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/104)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/235)
:end_tab:
