```{.python .input}
%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow'])
```

# 多層パーセプトロン
:label:`sec_mlp`

:numref:`chap_classification`では、アルゴリズムをゼロから実装し（:numref:`sec_softmax_scratch`）、高レベルAPI（:numref:`sec_softmax_concise`）を使用して、ソフトマックス回帰（:numref:`sec_softmax`）を導入しました。これにより、低解像度の画像から衣類の10種類を認識できる分類器をトレーニングすることができました。その過程で、データをまとめ、出力を有効な確率分布に強制し、適切な損失関数を適用し、モデルのパラメーターに関して最小化する方法を学びました。単純な線形モデルのコンテキストでこれらのメカニズムを習得したので、この本が主に関係する比較的豊富なクラスのモデルであるディープニューラルネットワークの探索を開始できます。 

## 非表示レイヤー

:numref:`subsec_linear_model`では、アフィン変換をバイアスを加えた線形変換として説明しました。はじめに、:numref:`fig_softmaxreg`に示されているソフトマックス回帰の例に対応するモデルアーキテクチャを思い出してください。このモデルは、単一のアフィン変換とそれに続くソフトマックス演算によって、入力を出力に直接マッピングします。ラベルが単純なアフィン変換によって本当に入力データに関連している場合、このアプローチで十分です。ただし、線形性 (アフィン変換における) は「強固な」仮定です。 

### 線形モデルの限界

たとえば、線形性は、*単調性*の*弱い*仮定を意味します。つまり、特徴量が増加すると、常にモデルの出力が増加する（対応する重みが正の場合）か、モデルの出力が常に減少する（対応する重みが負の場合）必要があります。時にはそれが理にかなっています。たとえば、個人がローンを返済するかどうかを予測しようとした場合、他のすべてのものが等しいと合理的に想定できます。収入の高い申請者は、低所得の申請者よりも常に返済する可能性が高くなります。単調ではあるが、この関係は返済の確率と直線的に関連していない可能性が高い。$0 to \\$50,000円からの収入の増加は、$1 million to \\$10万円からの増加よりも返済の可能性の大きな増加に相当する可能性が高い。これを処理する1つの方法は、ロジスティックマップ（および結果の確率の対数）を使用して、線形性がより妥当になるように結果を後処理することです。 

単調性に違反する例を簡単に思いつくことができることに注意してください。たとえば、体温の関数として健康を予測したいとします。体温が37°C（98.6°F）を超える個人では、体温が高いほどリスクが高いことを示します。ただし、体温が37°C未満の個人では、体温が低いほどリスクが高いことを示します。繰り返しになりますが、37°Cからの距離を特徴として使用するなど、巧妙な前処理で問題を解決できるかもしれません。 

しかし、猫と犬の画像を分類するのはどうですか？位置（13、17）のピクセルの強度を上げると、画像が犬を描写する可能性が常に高くなる（または常に減少する）必要がありますか？線形モデルへの依存は、猫と犬を区別するための唯一の要件は個々のピクセルの明るさを評価することであるという暗黙の仮定に対応します。このアプローチは、画像を反転させることでカテゴリが保持される世界では失敗する運命にあります。 

それでも、ここでの直線性は明らかに不条理ですが、前の例と比較して、単純な前処理の修正で問題に対処できるかどうかはあまり明白ではありません。これは、ピクセルの重要度は、そのコンテキスト（周囲のピクセルの値）に複雑に依存するためです。フィーチャ間の関連する相互作用を考慮したデータの表現が存在する可能性があり、その上に線形モデルが適していますが、それを手作業で計算する方法はわかりません。ディープニューラルネットワークでは、観測データを使用して、隠れ層を介した表現と、その表現に作用する線形予測変数の両方を共同で学習しました。 

この非線形性の問題は、少なくとも1世紀にわたって研究されてきた :cite:`Fisher.1928`。たとえば、最も基本的な形式の決定木は、クラスのメンバーシップを決定するために一連のバイナリ決定を使用します :cite:`quinlan2014c4`。同様に、カーネル法は非線形依存関係をモデル化するために何十年も前から使用されてきました :cite:`Aronszajn.1950`。これは、例えば、ノンパラメトリックスプラインモデル:cite:`Wahba.1990`とカーネル法:cite:`Scholkopf.Smola.2002`への道を見つけました。それはまた、脳がかなり自然に解決するものです。結局のところ、ニューロンは他のニューロンに供給され、次に他のニューロンに再び供給されます :cite:`Cajal.Azoulay.1894`。その結果、一連の比較的単純な変換があります。 

### 隠しレイヤーを組み込む

1つ以上の隠れ層を組み込むことで、線形モデルの限界を克服できます。これを行う最も簡単な方法は、完全に接続された多数のレイヤーを互いに積み重ねることです。各レイヤーは、出力を生成するまで、その上のレイヤーに入力されます。最初の $L-1$ 層は表現として、最後の層は線形予測子と考えることができます。このアーキテクチャは一般に*多層パーセプトロン*と呼ばれ、しばしば*MLP* (:numref:`fig_mlp`) と略されます。 

![An MLP with a hidden layer of 5 hidden units. ](../img/mlp.svg)
:label:`fig_mlp`

この MLP には 4 つの入力、3 つの出力があり、その隠れ層には 5 つの隠れユニットが含まれています。入力層には計算が含まれないため、このネットワークで出力を生成するには、隠れ層と出力層の両方の計算を実装する必要があります。したがって、この MLP の層数は 2 です。両方のレイヤーが完全に接続されていることに注意してください。すべての入力は隠れ層のすべてのニューロンに影響し、これらの各入力は出力層のすべてのニューロンに影響を与えます。悲しいかな、まだ終わっていません。 

### 線形から非線形へ

前述のように、$n$の例のミニバッチを行列$\mathbf{X} \in \mathbb{R}^{n \times d}$で表します。各例には$d$の入力（特徴）があります。隠れ層が$h$の隠れユニットを持つ1つの隠れ層MLPの場合、$\mathbf{H} \in \mathbb{R}^{n \times h}$によって隠れ層の出力を示します。
*非表示の表現*。
隠れ層と出力層の両方が完全に接続されているため、隠れ層の重み $\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$ とバイアス $\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$ と出力層の重み $\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$ とバイアス $\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$ があります。これにより、1 隠れ層 MLP の出力 $\mathbf{O} \in \mathbb{R}^{n \times q}$ を次のように計算できます。 

$$
\begin{aligned}
    \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.
\end{aligned}
$$

非表示レイヤーを追加した後、モデルでは追加のパラメーターセットを追跡および更新する必要があることに注意してください。それでは、引き換えに何が得られましたか？上で定義したモデルでは、*トラブルに対して何も得られない*、と知って驚くかもしれません！その理由は明白です。上記の隠れ単位は入力のアフィン関数によって与えられ、出力 (pre-softmax) は隠れ単位の単なるアフィン関数です。アフィン関数のアフィン関数は、それ自体がアフィン関数です。さらに、私たちの線形モデルはすでにあらゆるアフィン関数を表すことができました。 

これを正式に見るには、上記の定義で隠れたレイヤーを折りたたむだけで、$\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}$と$\mathbf{b} = \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)}$のパラメーターを持つ同等の単層モデルが得られます。 

$$
\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}.
$$

多層アーキテクチャの可能性を実現するには、アフィン変換の後に隠れた各ユニットに適用する非線形*活性化関数* $\sigma$という重要な要素をもう1つ必要とします。たとえば、一般的な選択肢は、ReLU (整流線形単位) 活性化関数 :cite:`Nair.Hinton.2010` $\sigma(x) = \mathrm{max}(0, x)$ がその引数を要素単位で操作することです。アクティベーション関数 $\sigma(\cdot)$ の出力は、*アクティベーション* と呼ばれます。一般に、アクティベーション関数があれば、MLPを線形モデルに折りたたむことはできなくなりました。 

$$
\begin{aligned}
    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
\end{aligned}
$$

$\mathbf{X}$ の各行はミニバッチの例に対応し、表記法の乱用があるため、非線形性 $\sigma$ を行単位で、つまり一度に 1 つの例として入力に適用するように定義します。:numref:`subsec_softmax_vectorization` で行単位の演算を示すとき、softmax にも同じ表記を使用したことに注意してください。私たちが使用するアクティベーション関数は、行単位だけでなく要素単位にも適用されることがよくあります。つまり、レイヤーの線形部分を計算した後、他の隠れユニットが取った値を見ることなく、各アクティベーションを計算できます。 

より一般的なMLPを構築するために、$\mathbf{H}^{(1)} = \sigma_1(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})$や$\mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)})$などの隠れたレイヤーを積み重ね続け、より表現力豊かなモデルを生み出すことができます。 

### ユニバーサル近似器

脳は非常に高度な統計分析が可能であることを私たちは知っています。そのため、ディープネットワークがどれほど強力である可能性があるかを尋ねる価値があります。この質問は何度も回答されています。たとえば、MLPのコンテキストでは:citet:`Cybenko.1989`で、単一の隠れ層を持つ放射状基底関数（RBF）ネットワークと見なすことができる方法でカーネルヒルベルト空間を再現するコンテキストでは:citet:`micchelli1984interpolation`です。これら（および関連する結果）は、十分な数のノード（おそらく不合理な数）と適切な重みのセットが与えられれば、単一の隠れ層ネットワークであっても、任意の関数をモデル化できることを示唆しています。しかし、実際にその機能を学ぶのは難しい部分です。ニューラルネットワークはCプログラミング言語に少し似ていると考えるかもしれません。この言語は、他の現代言語と同様に、あらゆる計算可能なプログラムを表現することができます。しかし、実際にあなたの仕様に合ったプログラムを思いつくのは難しい部分です。 

しかも、単一の隠れ層ネットワークだからこそ
*どんな機能でも学べる*
は、単一隠れ層ネットワークに関する問題をすべて解決しようとすべきだという意味ではありません。実際、この場合、カーネルメソッドは問題を解決できるため、はるかに効果的です。
*無限次元空間でも正確* :cite:`Kimeldorf.Wahba.1971,Scholkopf.Herbrich.Smola.2001`。
実際、より深い（より広い）ネットワークを使用することで、多くの関数をはるかにコンパクトに近似できます。:cite:`Simonyan.Zisserman.2014`。以降の章では、より厳密な議論に触れます。 

## アクティベーション機能
:label:`subsec_activation-functions`

活性化関数は、加重和を計算し、さらにバイアスを加えることによって、ニューロンを活性化すべきかどうかを決定します。これらは入力信号を出力に変換する微分可能な演算子ですが、そのほとんどは非線形性を追加します。アクティベーション関数はディープラーニングの基本であるため、(**いくつかの一般的なアクティベーション関数を簡単に調べてみましょう**)。

```{.python .input}
%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
npx.set_np()
```

```{.python .input}
%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
```

```{.python .input}
%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
```

### ReLU 関数

実装の単純さとさまざまな予測タスクでの優れたパフォーマンスの両方から、最も人気のある選択肢は、*整流線形単位* (*ReLU*) :cite:`Nair.Hinton.2010`です。[**ReLUは非常に単純な非線形変換を提供します**]。要素$x$が与えられると、関数はその要素の最大値と$0$として定義されます。 

$$\operatorname{ReLU}(x) = \max(x, 0).$$

非公式には、ReLU 関数は正の要素のみを保持し、対応するアクティブ化を 0 に設定することですべての負の要素を破棄します。ある程度の直感を得るために、関数をプロットできます。ご覧のとおり、活性化関数は区分線形です。

```{.python .input}
%%tab mxnet
x = np.arange(-8.0, 8.0, 0.1)
x.attach_grad()
with autograd.record():
    y = npx.relu(x)
d2l.plot(x, y, 'x', 'relu(x)', figsize=(5, 2.5))
```

```{.python .input}
%%tab pytorch
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
```

```{.python .input}
%%tab tensorflow
x = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32)
y = tf.nn.relu(x)
d2l.plot(x.numpy(), y.numpy(), 'x', 'relu(x)', figsize=(5, 2.5))
```

入力が負の場合、ReLU 関数の微分は 0 で、入力が正の場合、ReLU 関数の微分は 1 です。入力が正確に 0 と等しい値を取る場合、ReLU 関数は微分できないことに注意してください。このような場合、デフォルトは左辺の微分で、入力が0のときに微分が0であるとします。入力が実際にはゼロになることはないかもしれないので、これを回避できます（数学者は、メジャーゼロのセットでは微分不可能だと言うでしょう）。微妙な境界条件が重要な場合、私たちはおそらく工学ではなく（*本物*）数学をしているという古い格言があります。この従来の知恵は、ここで当てはまるかもしれません。少なくとも、制約付き最適化を実行していないという事実に当てはまるかもしれません :cite:`Mangasarian.1965,Rockafellar.1970`。以下にプロットした ReLU 関数の微分をプロットします。

```{.python .input}
%%tab mxnet
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
```

```{.python .input}
%%tab pytorch
y.backward(torch.ones_like(x), retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
```

```{.python .input}
%%tab tensorflow
with tf.GradientTape() as t:
    y = tf.nn.relu(x)
d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of relu',
         figsize=(5, 2.5))
```

ReLUを使用する理由は、その派生物が特にうまく動作するためです。それらは消滅するか、単に議論を通過させるかのどちらかです。これにより、最適化の動作が向上し、以前のバージョンのニューラルネットワークを悩ませていた勾配が消失するという十分に文書化された問題が軽減されました（これについては後で詳しく説明します）。 

*パラメータ化された ReLU* (*preLU*) 関数 :cite:`He.Zhang.Ren.ea.2015` など、ReLU 関数には多くのバリエーションがあることに注意してください。この変化は ReLU に線形項を追加するため、引数が負の場合でも、一部の情報は引き継がれます。 

$$\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).$$

### シグモイド関数

[***シグモイド関数* は入力を変換します**]、その値は領域 $\mathbb{R}$ にあります (**区間 (0, 1) にある出力に。**) そのため、シグモイドはしばしば*押し潰し関数* と呼ばれます:範囲 (-inf, inf) の入力を (0, 1) の範囲内の値に押しつぶします。 

$$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$$

初期のニューラルネットワークでは、科学者は*発火*または*発火しない*生体ニューロンをモデル化することに興味を持っていました。したがって、この分野の先駆者たちは、人工ニューロンの発明者であるマカロックとピッツにまでさかのぼり、しきい値処理ユニット:cite:`McCulloch.Pitts.1943`に焦点を当てました。スレッショルディングのアクティブ化は、入力があるスレッショルドを下回る場合は値0、入力がスレッショルドを超えると値1になります。 

勾配ベースの学習に注目が移ったとき、シグモイド関数はしきい値単位に対する滑らかで微分可能な近似であるため、自然な選択でした。シグモイドは、出力をバイナリ分類問題の確率として解釈する場合、出力単位の活性化関数として今でも広く使用されています。シグモイドはソフトマックスの特殊なケースと考えることができます。しかし、シグモイドは、隠れ層でのほとんどの使用のために、より単純で訓練しやすいReLUにほとんど置き換えられています。これの多くは、シグモイドが最適化の課題となるという事実に関係しています :cite:`LeCun.Bottou.Orr.ea.1998`。これは、その勾配が大きな正の*および*の負の引数に対して消失するためです。これは、脱出するのが難しい高原につながる可能性があります。それにもかかわらず、シグモイドは重要です。リカレントニューラルネットワークに関する後の章（例：:numref:`sec_lstm`）では、シグモイドユニットを活用して時間の経過に伴う情報の流れを制御するアーキテクチャについて説明します。 

以下に、シグモイド関数をプロットします。入力が 0 に近い場合、シグモイド関数は線形変換に近づくことに注意してください。

```{.python .input}
%%tab mxnet
with autograd.record():
    y = npx.sigmoid(x)
d2l.plot(x, y, 'x', 'sigmoid(x)', figsize=(5, 2.5))
```

```{.python .input}
%%tab pytorch
y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
```

```{.python .input}
%%tab tensorflow
y = tf.nn.sigmoid(x)
d2l.plot(x.numpy(), y.numpy(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
```

シグモイド関数の微分は次の方程式で与えられます。 

$$\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).$$

シグモイド関数の導関数を以下にプロットします。入力が 0 の場合、シグモイド関数の微分は最大 0.25 に達することに注意してください。入力が 0 からいずれかの方向に発散すると、微分は 0 に近づきます。

```{.python .input}
%%tab mxnet
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
```

```{.python .input}
%%tab pytorch
# Clear out previous gradients
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
```

```{.python .input}
%%tab tensorflow
with tf.GradientTape() as t:
    y = tf.nn.sigmoid(x)
d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of sigmoid',
         figsize=(5, 2.5))
```

### タン機能
:label:`subsec_tanh`

シグモイド関数と同様に、[**tanh (双曲線正接) 関数も入力を押しつぶします**]、区間 (**-1 と 1 の間**) の要素に変換します。 

$$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$$

以下に tanh 関数をプロットします。入力が 0 に近づくにつれ、関数 tanh は線形変換に近づくことに注意してください。関数の形状はシグモイド関数の形状と似ていますが、tanh 関数は座標系 :cite:`Kalman.Kwasny.1992` の原点を中心に点対称になります。

```{.python .input}
%%tab mxnet
with autograd.record():
    y = np.tanh(x)
d2l.plot(x, y, 'x', 'tanh(x)', figsize=(5, 2.5))
```

```{.python .input}
%%tab pytorch
y = torch.tanh(x)
d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))
```

```{.python .input}
%%tab tensorflow
y = tf.nn.tanh(x)
d2l.plot(x.numpy(), y.numpy(), 'x', 'tanh(x)', figsize=(5, 2.5))
```

tanh 関数の導関数は次のとおりです。 

$$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$$

下にプロットされています。入力が 0 に近づくにつれて、関数 tanh の微分は最大 1 に近づきます。シグモイド関数で見たように、入力がいずれかの方向に 0 から離れるにつれて、tanh 関数の微分は 0 に近づきます。

```{.python .input}
%%tab mxnet
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
```

```{.python .input}
%%tab pytorch
# Clear out previous gradients.
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
```

```{.python .input}
%%tab tensorflow
with tf.GradientTape() as t:
    y = tf.nn.tanh(x)
d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of tanh',
         figsize=(5, 2.5))
```

## まとめ

私たちは今、表現力豊かな多層ニューラルネットワークアーキテクチャを構築するために非線形性を組み込む方法を知っています。補足として、あなたの知識はすでに1990年頃の開業医と同様のツールキットを指揮しています。ある意味では、強力なオープンソースのディープラーニングフレームワークを活用して、わずか数行のコードでモデルを迅速に構築できるため、1990年代に働く誰よりも有利です。以前は、これらのネットワークをトレーニングするには、C、Fortran、または Lisp (LeNet の場合) でレイヤーとデリバティブを明示的にコード化する必要がありました。 

二次的な利点は、ReLU がシグモイドや tanh 関数よりも最適化にかなり適していることです。これは、過去10年間にディープラーニングが復活するのを助けた重要な革新の1つだったと言えるでしょう。ただし、アクティベーション機能の研究は止まっていないことに注意してください。たとえば、:cite:`Ramachandran.Zoph.Le.2017`で提案されているSwishアクティベーション関数$\sigma(x) = x \operatorname{sigmoid}(\beta x)$は、多くの場合、より高い精度を得ることができます。 

## 演習

1. *線形*の深いネットワーク、つまり非線形性のないネットワークに層を追加しても、ネットワークの表現力を高めることはできないことを示してください。それが積極的にそれを減らす例を挙げてください。
1. PreLU 活性化関数の微分を計算します。
1. スウィッシュ活性化関数 $x \operatorname{sigmoid}(\beta x)$ の微分を計算します。
1. ReLU (または preLU) のみを使用する MLP が連続区分的線形関数を構成することを示します。
1. シグモイドとタンは非常に似ています。
    1. $\operatorname{tanh}(x) + 1 = 2 \operatorname{sigmoid}(2x)$を見せて。
    1. 両方の非線形性によってパラメーター化された関数クラスが同一であることを証明します。ヒント:アフィン層にもバイアス項があります。
1. バッチ正規化 :cite:`Ioffe.Szegedy.2015` のように、一度に 1 つのミニバッチに適用される非線形性があるとします。これによってどのような問題が発生すると予想されますか？
1. シグモイド活性化関数の勾配が消失する例を挙げてください。

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/90)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/91)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/226)
:end_tab:
