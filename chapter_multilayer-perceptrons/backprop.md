# フォワードプロパゲーション、バックワードプロパゲーション、および計算グラフ
:label:`sec_backprop`

これまで、ミニバッチの確率的勾配降下法でモデルをトレーニングしてきました。しかし、アルゴリズムを実装したときは、モデルを介した*前方伝播*に関連する計算のみを懸念していました。勾配を計算するときが来たとき、ディープラーニングフレームワークによって提供されるバックプロパゲーション関数を呼び出しました。 

勾配の自動計算（自動微分）により、ディープラーニングアルゴリズムの実装が大幅に簡素化されます。自動微分以前は、複雑なモデルに少しでも変更を加えるだけでも、複雑な微分を手動で再計算する必要がありました。驚くべきことに、学術論文は更新規則を導き出すために多数のページを割り当てる必要がありました。興味深い部分に集中できるように自動微分に依存し続ける必要がありますが、ディープラーニングの浅い理解を超えたい場合は、これらの勾配が内部でどのように計算されるかを知っておく必要があります。 

このセクションでは、*逆伝播* (より一般的には*バックプロパゲーション*) の詳細を掘り下げます。技術とその実装の両方についてある程度の洞察を伝えるために、私たちはいくつかの基本的な数学と計算グラフに依存しています。まず、重量減衰を伴う1つの隠れ層MLPに焦点を当てます（$\ell_2$正則化、後続の章で説明します）。 

## フォワードプロパゲーション

*フォワードプロパゲーション* (または*フォワードパス*) は、計算とストレージを指します
ニューラルネットワークの中間変数（出力を含む）を、入力層から出力層の順に並べて表示します。ここでは、隠れ層が 1 つあるニューラルネットワークの仕組みを段階的に説明していきます。これは退屈に思えるかもしれませんが、ファンクの巨匠ジェームズ・ブラウンの永遠の言葉では、あなたは「ボスになるための費用を支払う」必要があります。 

簡単にするために、入力例が$\mathbf{x}\in \mathbb{R}^d$であり、隠れ層にバイアス項が含まれていないと仮定します。ここで、中間変数は次のとおりです。 

$$\mathbf{z}= \mathbf{W}^{(1)} \mathbf{x},$$

ここで、$\mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$は非表示レイヤーの重みパラメータです。活性化関数$\phi$を介して中間変数$\mathbf{z}\in \mathbb{R}^h$を実行した後、長さ$h$の隠れ活性化ベクトルが得られます。 

$$\mathbf{h}= \phi (\mathbf{z}).$$

隠れ層出力 $\mathbf{h}$ も中間変数です。出力層のパラメーターが $\mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$ の重みしか持たないと仮定すると、長さ $q$ のベクトルを持つ出力層変数を取得できます。 

$$\mathbf{o}= \mathbf{W}^{(2)} \mathbf{h}.$$

損失関数が$l$で、例のラベルが$y$であると仮定すると、単一のデータ例の損失項を計算できます。 

$$L = l(\mathbf{o}, y).$$

後で紹介する $\ell_2$ の正則化の定義によれば、ハイパーパラメータ $\lambda$ を考えると、正則化項は次のようになります。 

$$s = \frac{\lambda}{2} \left(\|\mathbf{W}^{(1)}\|_F^2 + \|\mathbf{W}^{(2)}\|_F^2\right),$$
:eqlabel:`eq_forward-s`

ここで、行列のフロベニウスノルムは、行列をベクトルに平坦化した後に適用される $\ell_2$ ノルムです。最後に、特定のデータ例に対するモデルの正則化された損失は次のとおりです。 

$$J = L + s.$$

次の説明では、$J$を*目的関数*と呼びます。 

## 前方伝播の計算グラフ

*計算グラフ*をプロットすると、計算内の演算子と変数の依存関係を視覚化するのに役立ちます。:numref:`fig_forward`には、上で説明した単純なネットワークに関連するグラフが含まれており、正方形は変数を表し、円は演算子を表します。左下隅は入力を表し、右上隅は出力を示します。矢印 (データフローを示す) の方向は、主に右方向と上向きであることに注意してください。 

![Computational graph of forward propagation.](../img/forward.svg)
:label:`fig_forward`

## バックプロパゲーション

*バックプロパゲーション* は計算方法を指します
ニューラルネットワークパラメータの勾配。要するに、この方法は、微積分からの*連鎖法*に従って、出力層から入力層まで逆の順序でネットワークを横断します。このアルゴリズムは、一部のパラメーターに関する勾配を計算するときに必要な中間変数 (偏導関数) を保存します。入力と出力$\mathsf{X}, \mathsf{Y}, \mathsf{Z}$が任意の形状のテンソルである関数$\mathsf{Y}=f(\mathsf{X})$と$\mathsf{Z}=g(\mathsf{Y})$があると仮定します。連鎖則を使用することにより、次の方法で$\mathsf{X}$に対する$\mathsf{Z}$の微分を計算できます。 

$$\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \text{prod}\left(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}}\right).$$

ここでは、$\text{prod}$演算子を使用して、転置や入力位置の入れ替えなどの必要な操作が実行された後、引数を乗算します。ベクトルの場合、これは簡単です。単純に行列と行列の乗算です。高次元のテンソルには、適切なテンソルを使用します。演算子 $\text{prod}$ は、すべての表記オーバーヘッドを隠します。 

計算グラフが :numref:`fig_forward` にある 1 つの隠れ層を持つ単純ネットワークのパラメーターは、$\mathbf{W}^{(1)}$ と $\mathbf{W}^{(2)}$ であることを思い出してください。バックプロパゲーションの目的は、$\partial J/\partial \mathbf{W}^{(1)}$ と $\partial J/\partial \mathbf{W}^{(2)}$ の勾配を計算することです。これを達成するために、連鎖則を適用し、各中間変数とパラメータの勾配を計算します。計算の順序は、順伝播で実行される順序と逆になります。これは、計算グラフの結果から始めて、パラメーターに向かって進む必要があるためです。最初のステップは、損失項 $L$ と正則化項 $s$ に対する目的関数 $J=L+s$ の勾配を計算することです。 

$$\frac{\partial J}{\partial L} = 1 \; \text{and} \; \frac{\partial J}{\partial s} = 1.$$

次に、連鎖則に従って出力層 $\mathbf{o}$ の変数に対する目的関数の勾配を計算します。 

$$
\frac{\partial J}{\partial \mathbf{o}}
= \text{prod}\left(\frac{\partial J}{\partial L}, \frac{\partial L}{\partial \mathbf{o}}\right)
= \frac{\partial L}{\partial \mathbf{o}}
\in \mathbb{R}^q.
$$

次に、両方のパラメータに関する正則化項の勾配を計算します。 

$$\frac{\partial s}{\partial \mathbf{W}^{(1)}} = \lambda \mathbf{W}^{(1)}
\; \text{and} \;
\frac{\partial s}{\partial \mathbf{W}^{(2)}} = \lambda \mathbf{W}^{(2)}.$$

これで、出力層に最も近いモデルパラメーターの勾配 $\partial J/\partial \mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$ を計算できます。連鎖ルールを使用すると、次の結果が得られます。 

$$\frac{\partial J}{\partial \mathbf{W}^{(2)}}= \text{prod}\left(\frac{\partial J}{\partial \mathbf{o}}, \frac{\partial \mathbf{o}}{\partial \mathbf{W}^{(2)}}\right) + \text{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \mathbf{W}^{(2)}}\right)= \frac{\partial J}{\partial \mathbf{o}} \mathbf{h}^\top + \lambda \mathbf{W}^{(2)}.$$
:eqlabel:`eq_backprop-J-h`

$\mathbf{W}^{(1)}$に関する勾配を得るには、出力層に沿って隠れ層への逆伝播を続ける必要があります。隠れ層出力 $\partial J/\partial \mathbf{h} \in \mathbb{R}^h$ に対する勾配は、次の式で与えられます。 

$$
\frac{\partial J}{\partial \mathbf{h}}
= \text{prod}\left(\frac{\partial J}{\partial \mathbf{o}}, \frac{\partial \mathbf{o}}{\partial \mathbf{h}}\right)
= {\mathbf{W}^{(2)}}^\top \frac{\partial J}{\partial \mathbf{o}}.
$$

活性化関数 $\phi$ は要素単位に適用されるため、中間変数 $\mathbf{z}$ の勾配 $\partial J/\partial \mathbf{z} \in \mathbb{R}^h$ を計算するには、要素単位の乗算演算子を使用する必要があります。これは $\odot$ で表します。 

$$
\frac{\partial J}{\partial \mathbf{z}}
= \text{prod}\left(\frac{\partial J}{\partial \mathbf{h}}, \frac{\partial \mathbf{h}}{\partial \mathbf{z}}\right)
= \frac{\partial J}{\partial \mathbf{h}} \odot \phi'\left(\mathbf{z}\right).
$$

最後に、入力層に最も近いモデルパラメーターの勾配 $\partial J/\partial \mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$ を取得できます。連鎖ルールによると、私たちは 

$$
\frac{\partial J}{\partial \mathbf{W}^{(1)}}
= \text{prod}\left(\frac{\partial J}{\partial \mathbf{z}}, \frac{\partial \mathbf{z}}{\partial \mathbf{W}^{(1)}}\right) + \text{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \mathbf{W}^{(1)}}\right)
= \frac{\partial J}{\partial \mathbf{z}} \mathbf{x}^\top + \lambda \mathbf{W}^{(1)}.
$$

## ニューラルネットワークのトレーニング

ニューラルネットワークを学習させる場合、順伝播と逆伝播は互いに依存します。特に、順伝播では、計算グラフを依存関係の方向にトラバースし、そのパス上のすべての変数を計算します。これらは、グラフ上の計算順序が逆になるバックプロパゲーションに使用されます。 

説明する例として、前述の単純なネットワークを取り上げます。一方では、順伝播中の正則化項 :eqref:`eq_forward-s` の計算は、モデルパラメーター $\mathbf{W}^{(1)}$ および $\mathbf{W}^{(2)}$ の現在の値に依存します。これらは、最新の反復におけるバックプロパゲーションに従って最適化アルゴリズムによって与えられます。一方、バックプロパゲーション中のパラメーター :eqref:`eq_backprop-J-h` の勾配計算は、フォワードプロパゲーションによって与えられる隠れ層出力 $\mathbf{h}$ の現在の値に依存します。 

したがって、ニューラルネットワークをトレーニングする場合、モデルパラメーターが初期化された後、順伝播とバックプロパゲーションを交互に行い、バックプロパゲーションによって与えられる勾配を使用してモデルパラメーターを更新します。バックプロパゲーションでは、計算の重複を避けるために、格納されている中間値が順伝播から再利用されることに注意してください。その結果の 1 つは、バックプロパゲーションが完了するまで中間値を保持する必要があることです。これは、トレーニングが単純な予測よりもはるかに多くのメモリを必要とする理由の1つでもあります。また、このような中間値のサイズは、ネットワーク層の数とバッチサイズにほぼ比例します。したがって、より大きなバッチサイズを使用してより深いネットワークを学習させると、*メモリ不足* エラーが発生しやすくなります。 

## まとめ

* 前方伝播は、ニューラルネットワークによって定義された計算グラフ内の中間変数を順次計算して保存します。入力層から出力層に進みます。
* バックプロパゲーションは、ニューラルネットワーク内の中間変数とパラメーターの勾配を逆の順序で順次計算して保存します。
* ディープラーニングモデルをトレーニングする場合、フォワードプロパゲーションとバックプロパゲーションは相互に依存しています。
* トレーニングには、予測よりもはるかに多くのメモリが必要です。

## 演習

1. いくつかのスカラー関数 $f$ への入力 $\mathbf{X}$ が $n \times m$ 行列であると仮定します。$\mathbf{X}$に対する$f$の勾配の次元はどれくらいですか？
1. このセクションで説明するモデルの隠れ層にバイアスを追加します (正則化項にバイアスを含める必要はありません)。
    1. 対応する計算グラフを描画します。
    1. 順伝播方程式と逆伝播方程式を導出する。
1. このセクションで説明するモデルで、学習と予測のためのメモリフットプリントを計算します。
1. 二次導関数を計算すると仮定します。コンピュテーショナルグラフはどうなりますか？計算にはどれくらい時間がかかると思いますか？
1. 計算グラフが GPU に対して大きすぎると仮定します。
    1. それを複数の GPU に分割できますか？
    1. 小規模なミニバッチでのトレーニングに勝るメリットとデメリットは何ですか?

[Discussions](https://discuss.d2l.ai/t/102)
