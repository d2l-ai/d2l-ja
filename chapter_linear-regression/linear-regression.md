```{.python .input  n=1}
%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow'])
```

# 線形回帰
:label:`sec_linear_regression`

*数値を予測したいときはいつでも、回帰*の問題がポップアップします。
一般的な例には、（住宅、株などの）価格の予測、（入院中の患者の）滞在期間の予測、（小売販売のための）需要の予測など、数え切れないほどあります。すべての予測問題が古典的な回帰問題であるとは限りません。後で、分類問題を紹介します。ここでは、一連のカテゴリ間のメンバーシップを予測することが目的です。 

実行例として、面積（平方フィート）と年齢（年）に基づいて住宅の価格（ドル単位）を見積もるとします。住宅価格を予測するモデルを開発するには、各住宅の販売価格、面積、年齢などの売上高からなるデータを手に入れる必要があります。機械学習の用語では、データセットは*トレーニングデータセット*または*トレーニングセット*と呼ばれ、各行（1つの販売に対応するデータを含む）は*例*（または*データポイント*、*インスタンス*、*サンプル*）と呼ばれます。私たちが予測しようとしているもの（価格）は、*ラベル*（または*ターゲット*）と呼ばれます。予測の基になる変数（年齢と面積）は、*特徴*（または*共変量*）と呼ばれます。 

## 基本

*線形回帰*はどちらも最も簡単かもしれません
回帰問題に取り組むための標準的なツールの中で最も人気があります。19世紀:cite:`Legendre.1805,Gauss.1809`の夜明けにさかのぼる線形回帰は、いくつかの単純な仮定から流れます。まず、フィーチャ $\mathbf{x}$ とターゲット $y$ の関係がほぼ線形であると仮定します。つまり、条件付き平均 $E[Y \mid X=\mathbf{x}]$ は、フィーチャ $\mathbf{x}$ の重み付き和として表すことができます。この設定により、観測ノイズのために目標値が期待値から逸脱する可能性があります。次に、ガウス分布に従って、そのようなノイズが適切に動作するという仮定を課すことができます。通常、データセット内の例の数を示すために$n$を使用します。上付き文字を使用してサンプルとターゲットを列挙し、添字を使用して座標をインデックスします。具体的には、$\mathbf{x}^{(i)}$は$i$番目のサンプルを示し、$x_j^{(i)}$は$j$番目の座標を示します。 

### モデル
:label:`subsec_linear_model`

すべてのソリューションの中心には、フィーチャをターゲットの推定値に変換する方法を記述するモデルがあります。線形性の仮定は、ターゲット（価格）の期待値をフィーチャ（面積と年齢）の加重合計として表すことができることを意味します。 

$$\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b.$$
:eqlabel:`eq_price-area`

ここで、$w_{\mathrm{area}}$と$w_{\mathrm{age}}$は*重み*と呼ばれ、$b$は*バイアス*（または*オフセット*または*インターセプト*）と呼ばれます。重みは、予測に対する各特徴量の影響を決定します。バイアスは、すべての特徴量がゼロの場合の推定値を決定します。面積が正確にゼロの新しく建てられた家を見ることはありませんが、（原点を通る線に制限するのではなく）フィーチャのすべての線形関数を表現できるため、バイアスが必要です。厳密に言えば、:eqref:`eq_price-area`は入力フィーチャの*アフィン変換*であり、加重和によるフィーチャの*線形変換*と、追加されたバイアスによる*平行移動*の組み合わせによって特徴付けられます。データセットが与えられた場合、私たちの目標は、モデルの予測がデータで観察された真の価格にできるだけ近づくように、平均して重み$\mathbf{w}$とバイアス$b$を選択することです。 

いくつかの特徴量だけを持つデータセットに焦点を当てることが一般的な分野では、:eqref:`eq_price-area`のようにモデルを長い形式で明示的に表現するのが一般的です。機械学習では、通常、コンパクトな線形代数表記を使用する方が便利な高次元のデータセットを扱います。入力が$d$フィーチャで構成されている場合、それぞれにインデックス（$1$から$d$の間）を割り当て、予測$\hat{y}$（一般に「帽子」記号は推定値を示します）を次のように表現できます。 

$$\hat{y} = w_1  x_1 + ... + w_d  x_d + b.$$

すべての特徴量をベクトル $\mathbf{x} \in \mathbb{R}^d$ に集め、すべての重みをベクトル $\mathbf{w} \in \mathbb{R}^d$ に集めると、$\mathbf{w}$ と $\mathbf{x}$ の間のドット積によってモデルをコンパクトに表現できます。 

$$\hat{y} = \mathbf{w}^\top \mathbf{x} + b.$$
:eqlabel:`eq_linreg-y`

:eqref:`eq_linreg-y` では、ベクトル $\mathbf{x}$ は 1 つの例の特徴に対応します。$n$例のデータセット全体の特徴を*設計マトリックス* $\mathbf{X} \in \mathbb{R}^{n \times d}$で参照すると便利なことがよくあります。ここで、$\mathbf{X}$ には、例ごとに 1 つの行と、各フィーチャごとに 1 つの列が含まれています。特徴量$\mathbf{X}$の集合の場合、予測$\hat{\mathbf{y}} \in \mathbb{R}^n$は行列-ベクトル積によって表すことができます。 

$${\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b,$$

総和中に放送（:numref:`subsec_broadcasting`）が適用される場所。トレーニングデータセット$\mathbf{X}$の特徴と対応する（既知の）ラベル$\mathbf{y}$が与えられると、線形回帰の目標は、$\mathbf{X}$と同じ分布からサンプリングされた新しいデータ例の特徴を与える重みベクトル$\mathbf{w}$とバイアス項$b$を見つけることです。新しい例のラベルは（expectation）は最小の誤差で予測されます。 

$\mathbf{x}$が与えられた場合の$y$を予測するための最良のモデルが線形であると私たちが信じるとしても、$n$例の実世界のデータセットを見つけることは期待できません。$y^{(i)}$はすべて$1 \leq i \leq n$で$\mathbf{w}^\top \mathbf{x}^{(i)}+b$とまったく同じです。たとえば、$\mathbf{X}$とラベル$\mathbf{y}$を観察するために使用する機器が何であれ、わずかな測定誤差が生じる可能性があります。したがって、基礎となる関係が線形であると確信できる場合でも、そのような誤差を説明するためにノイズ項を組み込みます。 

最適な*パラメータ*（または*モデルパラメータ*）$\mathbf{w}$と$b$を検索する前に、（i）特定のモデルの品質尺度と、（ii）モデルを更新して品質を向上させる手順の2つが必要です。 

### 損失機能
:label:`subsec_linear-regression-loss-function`

当然、モデルをデータに適合させるには、*適合性*（または同等に*不適合性*）の尺度について合意する必要があります。
*損失関数* 距離を定量化する
ターゲットの*実数*と*予測*の値の間。通常、損失は非負の数であり、値が小さいほど優れており、完全な予測では0の損失が発生します。回帰問題の場合、最も一般的な損失関数は二乗誤差です。$i$の例に対する予測が$\hat{y}^{(i)}$で、対応する真のラベルが$y^{(i)}$の場合、*二乗誤差*は次のように求められます。 

$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$$
:eqlabel:`eq_mse`

定数$\frac{1}{2}$は実質的な違いはありませんが、損失の微分を取ると相殺されるため、表記上便利であることがわかります。トレーニングデータセットは私たちに与えられ、制御不能であるため、経験的誤差はモデルパラメータの関数にすぎません。以下では、一次元入力 (:numref:`fig_fit_linreg`) をもつ問題における線形回帰モデルの適合を可視化します。 

![Fitting a linear regression model to one-dimensional data.](../img/fit-linreg.svg)
:label:`fig_fit_linreg`

推定値$\hat{y}^{(i)}$と目標$y^{(i)}$の間の大きな違いは、損失の二次形式（これは両刃の剣である可能性があります）のために、損失へのより大きな寄与につながることに注意してください。これにより、モデルが大きなエラーを回避するよう促す一方で、異常なデータに対する過度の感度につながる可能性もあります）。$n$例のデータセット全体でモデルの品質を測定するには、トレーニングセットの損失を単純に平均（または同等に合計）します。 

$$L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$

モデルをトレーニングする場合、すべてのトレーニング例で合計損失を最小限に抑えるパラメーター ($\mathbf{w}^*, b^*$) を見つけます。 

$$\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).$$

### 分析ソリューション

これから取り上げるほとんどのモデルとは異なり、線形回帰は驚くほど簡単な最適化問題を提示します。特に、以下のような簡単な式を適用することにより、最適なパラメータ（トレーニングデータで評価される）を分析的に見つけることができます。まず、すべての 1 で構成される計画行列に列を追加することにより、バイアス $b$ をパラメーター $\mathbf{w}$ に含めることができます。次に、予測問題は $\|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2$ を最小化することです。設計行列 $\mathbf{X}$ がフルランクである限り（他のフィーチャに線形依存するフィーチャはありません）、損失曲面には臨界点が1つだけ存在し、ドメイン全体の損失の最小値に相当します。$\mathbf{w}$に関する損失の微分をゼロに設定すると、次のようになります。 

$$\begin{aligned}
    \partial_{\mathbf{w}} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 =
    2 \mathbf{X}^\top (\mathbf{X} \mathbf{w} - \mathbf{y}) = 0
    \text{ and hence }
    \mathbf{X}^\top \mathbf{y} = \mathbf{X}^\top \mathbf{X} \mathbf{w}.
\end{aligned}$$

$\mathbf{w}$を解くと、最適化問題の最適な解が得られます。この解決策に注意してください  

$$\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}$$

行列 $\mathbf X^\top \mathbf X$ が可逆である場合、つまり計画行列の列が線形独立である :cite:`Golub.Van-Loan.1996` の場合にのみ一意になります。 

線形回帰のような単純な問題は分析的な解決策を認めるかもしれませんが、そのような幸運に慣れるべきではありません。分析ソリューションは優れた数学的分析を可能にしますが、分析ソリューションの要件は非常に厳しく、ディープラーニングのエキサイティングな側面のほとんどすべてが除外されます。 

### ミニバッチ確率的勾配降下

幸いなことに、モデルを解析的に解くことができない場合でも、実際にはモデルを効果的にトレーニングできることがよくあります。さらに、多くのタスクでは、最適化が困難なモデルの方がはるかに優れているため、それらをどのようにトレーニングするかを考え出すことは、トラブルに見合うだけの価値があります。 

ほぼすべてのディープラーニングモデルを最適化するための重要な手法であり、この本全体で呼びますが、損失関数を段階的に下げる方向にパラメーターを更新することにより、エラーを繰り返し減らすことです。このアルゴリズムは*勾配降下*と呼ばれます。 

勾配降下法の最も単純な適用は、損失関数の導関数を取ることです。これは、データセット内のすべての例で計算された損失の平均です。実際には、これは非常に遅くなる可能性があります。更新ステップが非常に強力であっても、単一の更新を行う前にデータセット全体を渡す必要があります :cite:`Liu.Nocedal.1989`。さらに悪いことに、トレーニングデータに多くの冗長性がある場合、完全更新の利点はさらに低くなります。 

もう1つの極端な点は、一度に1つの例のみを検討し、一度に1つの観測値に基づいて更新手順を実行することです。結果として得られるアルゴリズムである*確率的勾配降下法* (SGD) は、大規模なデータセットに対しても効果的な戦略となります :cite:`Bottou.2010`。残念ながら、SGD には計算と統計の両方の欠点があります。1つの問題は、プロセッサがメインメモリからプロセッサキャッシュにデータを移動する場合よりも数値の乗算と加算がはるかに高速であるという事実から生じます。対応する数のベクトル-ベクトル演算よりも、行列-ベクトル乗算を実行する方が、最大で桁違いに効率的です。これは、完全なバッチと比較して、一度に1つのサンプルを処理するのに非常に長い時間がかかる可能性があることを意味します。2つ目の問題は、バッチ正規化（:numref:`sec_batch_norm`で説明）などの一部の層は、一度に複数の観測値にアクセスできる場合にのみうまく機能することです。 

両方の問題の解決策は、中間的な戦略を選択することです。完全なバッチまたは一度に1つのサンプルだけを取るのではなく、観測値の*ミニバッチ*を取る:cite:`Li.Zhang.Chen.ea.2014`。このミニバッチのサイズの具体的な選択は、メモリ量、アクセラレータの数、レイヤの選択、およびデータセットの合計サイズなど、多くの要因に依存します。それにもかかわらず、32から256の間の数、できれば$2$の大きな累乗の倍数が、良いスタートです。これにより、*ミニバッチの確率的勾配降下*につながります。 

最も基本的な形式では、各反復$t$で、最初に、固定数のトレーニング例$|\mathcal{B}|$で構成されるミニバッチ$\mathcal{B}_t$をランダムにサンプリングします。次に、モデルパラメーターに関するミニバッチの平均損失の微分 (勾配) を計算します。最後に、勾配に*学習率*と呼ばれるあらかじめ決められた小さな正の値 $\eta$ を掛け、現在のパラメーター値から結果の項を減算します。更新は次のように表現できます。 

$$(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).$$

要約すると、ミニバッチ SGD は次のように処理されます。(i) 通常はランダムにモデルパラメーターの値を初期化します。(ii) データからランダムなミニバッチを繰り返しサンプリングし、負の勾配の方向にパラメーターを更新します。二次損失とアフィン変換の場合、これは閉形式展開になります。 

$$\begin{aligned} \mathbf{w} & \leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) && = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)\\ b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \partial_b l^{(i)}(\mathbf{w}, b) &&  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}$$
:eqlabel:`eq_linreg_batch_update`

ミニバッチ$\mathcal{B}$を選ぶので、そのサイズ$|\mathcal{B}|$で正規化する必要があります。多くの場合、ミニバッチのサイズと学習率はユーザー定義です。トレーニングループで更新されないこのような調整可能なパラメーターは、*ハイパーパラメーター* と呼ばれます。これらは、ベイズ最適化 :cite:`Frazier.2018` など、さまざまな手法によって自動的に調整できます。最終的には、ソリューションの品質は通常、別の*検証データセット* (または*検証セット*) で評価されます。 

所定の反復回数（または他の停止基準が満たされるまで）のトレーニングの後、推定されたモデルパラメーター（$\hat{\mathbf{w}}, \hat{b}$）を記録します。関数が真に線形で、ノイズがない場合でも、これらのパラメータは損失の正確な最小化にはならず、決定論的でもないことに注意してください。アルゴリズムはミニマイザーに向かってゆっくりと収束しますが、通常、有限数のステップで正確に収束することはできません。さらに、パラメータを更新するために使用されるミニバッチ$\mathcal{B}$はランダムに選択されます。これは決定論を破る。 

線形回帰は、大域的最小値（$\mathbf{X}$がフルランクの場合は常に、または$\mathbf{X}^\top \mathbf{X}$が可逆である場合は同等）を伴う学習問題になります。ただし、ディープネットワークの損失曲面には多くのサドルポイントと最小値が含まれています。幸いなことに、私たちは通常、正確なパラメータセットを見つけることではなく、正確な予測につながる（したがって低損失）一連のパラメータを見つけることだけに関心があります。実際には、ディープラーニングの実践者は、*トレーニングセット* :cite:`Izmailov.Podoprikhin.Garipov.ea.2018,Frankle.Carbin.2018`の損失を最小限に抑えるパラメータを見つけるのに苦労することはめったにありません。より手ごわい作業は、これまで見られなかったデータの正確な予測につながるパラメーターを見つけることであり、これは*一般化*と呼ばれる課題です。本全体を通してこれらのトピックに戻ります。 

### 予測

モデル$\hat{\mathbf{w}}^\top \mathbf{x} + \hat{b}$を考えると、新しい例として*予測*を行うことができます。たとえば、面積$x_1$と年齢$x_2$を考えると、以前は見えなかった家の販売価格を予測します。ディープラーニングの実践者は、予測フェーズを「推論」と呼んでいますが、これはちょっとした誤称です。*推論* とは、パラメータの値と目に見えないインスタンスのありそうなラベルの両方を含む、証拠に基づいて到達した結論を広く指します。どちらかといえば、統計学の文献で
*inference* はパラメータ推論を示すことが多い
そして、この用語の過負荷は、ディープラーニングの実践者が統計学者と話すときに不必要な混乱を引き起こします。以下では、可能な限り*予測*に固執します。 

## 速度のためのベクトル化

モデルをトレーニングするとき、私たちは通常、サンプルのミニバッチ全体を同時に処理したいと考えています。これを効率的に行うには、(**we**) (~~should~~) (**計算をベクトル化し、Pythonで高価なfor-loopsを書くのではなく、高速な線形代数ライブラリを活用する**) が必要です。

```{.python .input  n=1}
%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import np
import time
```

```{.python .input  n=1}
%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import math
import torch
import numpy as np
import time
```

```{.python .input}
%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import math
import tensorflow as tf
import numpy as np
import time
```

なぜこれほど重要なのかを説明するために、(**ベクトルを加算する2つの方法を考えてみる**)、まず、すべてが 1 を含む 2 つの 10,000 次元のベクトルをインスタンス化します。ある方法では、Python の for ループでベクトルをループします。もう 1 つの方法では、`+` への 1 回の呼び出しに依存しています。

```{.python .input  n=2}
%%tab all
n = 10000
a = d2l.ones(n)
b = d2l.ones(n)
```

これで、ワークロードのベンチマークが可能になりました。まず、[**for-loopを使用して一度に1つの座標を追加します。**]

```{.python .input  n=3}
%%tab mxnet, pytorch
c = d2l.zeros(n)
t = time.time()
for i in range(n):
    c[i] = a[i] + b[i]
f'{time.time() - t:.5f} sec'
```

```{.python .input}
%%tab tensorflow
c = tf.Variable(d2l.zeros(n))
t = time.time()
for i in range(n):
    c[i].assign(a[i] + b[i])
f'{time.time() - t:.5f} sec'
```

(**あるいは、再ロードされた `+` 演算子を使用して要素単位の合計を計算します。**)

```{.python .input  n=4}
%%tab all
t = time.time()
d = a + b
f'{time.time() - t:.5f} sec'
```

2つ目の方法は、1つ目の方法よりも大幅に高速です。コードをベクトル化すると、多くの場合、桁違いに高速化されます。さらに、多くの計算を自分で記述する必要なく、より多くの数学をライブラリにプッシュし、エラーの可能性を減らし、コードの移植性を高めます。 

## 正規分布と二乗損失
:label:`subsec_normal_distribution_and_squared_loss`

ここまで、二乗損失の目的のかなり機能的な動機付けを与えてきました。最適なパラメーターは、基礎となるパターンが真に線形である場合は常に条件付き期待値 $E[Y\mid X]$ を返し、損失は外れ値に対して特大のペナルティを割り当てます。また、ノイズの分布について確率論的な仮定を行うことで、二乗損失の目標に対してより正式な動機を与えることもできます。 

線形回帰は、19世紀の変わり目に発明されました。ガウスとルジャンドルのどちらが最初にこの考えを考案したのかは長い間議論されてきましたが、正規分布（*ガウス*とも呼ばれる）も発見したのはガウスでした。正規分布と二乗損失を伴う線形回帰は、一般的な親子関係よりも深いつながりを共有していることがわかります。 

はじめに、平均が$\mu$、分散が$\sigma^2$（標準偏差$\sigma$）の正規分布は次のように与えられることを思い出してください。 

$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).$$

以下 [**正規分布を計算する関数を定義します**]。

```{.python .input  n=3}
%%tab all
def normal(x, mu, sigma):
    p = 1 / math.sqrt(2 * math.pi * sigma**2)
    return p * np.exp(-0.5 * (x - mu)**2 / sigma**2)
```

これで (**正規分布を可視化する**) ことができます。

```{.python .input  n=8}
%%tab mxnet
# Use numpy again for visualization
x = np.arange(-7, 7, 0.01)

# Mean and standard deviation pairs
params = [(0, 1), (0, 2), (3, 1)]
d2l.plot(x.asnumpy(), [normal(x, mu, sigma).asnumpy() for mu, sigma in params], xlabel='x',
         ylabel='p(x)', figsize=(4.5, 2.5),
         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])
```

```{.python .input  n=8}
%%tab pytorch, tensorflow
# Use numpy again for visualization
x = np.arange(-7, 7, 0.01)

# Mean and standard deviation pairs
params = [(0, 1), (0, 2), (3, 1)]
d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',
         ylabel='p(x)', figsize=(4.5, 2.5),
         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])
```

平均値の変化は$x$軸に沿ったシフトに対応し、分散を増やすと分布が広がり、ピークが下がることに注意してください。 

損失の二乗による線形回帰を動機付ける1つの方法は、観測値がノイズの多い測定値から発生すると仮定することです。ノイズは次のように正規分布しています。 

$$y = \mathbf{w}^\top \mathbf{x} + b + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, \sigma^2).$$

したがって、特定の$y$について、特定の$y$を見る*可能性*を次の方法で書き出すことができます。 

$$P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).$$

そのため、尤度は因数分解されます。*最尤法の原則*によると、パラメータ$\mathbf{w}$と$b$の最良値は、データセット全体の*尤度*を最大化する値です。 

$$P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)} \mid \mathbf{x}^{(i)}).$$

すべてのペア$(\mathbf{x}^{(i)}, y^{(i)})$が互いに独立して描画されたため、等価性が続きます。最尤法の原理に従って選択された推定量は、*最尤推定量*と呼ばれます。多くの指数関数の積を最大化するのは難しいように思えるかもしれませんが、代わりに尤度の対数を最大化することで、目的を変えずに物事を大幅に単純化できます。歴史的な理由から、最適化は最大化ではなく最小化として表現されることが多いです。したがって、何も変更せずに、*負の対数尤度*を*最小化*できます。これは次のように表現できます。 

$$-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.$$

$\sigma$が固定であると仮定すると、最初の項は無視できます。これは、$\mathbf{w}$または$b$に依存しないためです。2 番目の項は、乗法定数 $\frac{1}{\sigma^2}$ を除いて、前に導入した二乗誤差損失と同じです。幸いなことに、このソリューションは$\sigma$にも依存しません。したがって、平均二乗誤差を最小化することは、加法性ガウスノイズを仮定した場合の線形モデルの最尤推定と等価です。 

## ニューラルネットワークとしての線形回帰

線形モデルは、この本で紹介する多くの複雑なニューラルネットワークを表現するのに十分なほど豊富ではありませんが、ニューラルネットワークは、すべての特徴が入力ニューロンによって表され、そのすべてが出力に直接接続されているニューラルネットワークとして線形モデルを包含するのに十分なほど豊富です。 

:numref:`fig_single_neuron`は、線形回帰をニューラルネットワークとして表しています。この図は、各入力が出力にどのように接続されているかなどの接続パターンを強調していますが、重みやバイアスによって取られる特定の値は強調していません。 

![Linear regression is a single-layer neural network.](../img/singleneuron.svg)
:label:`fig_single_neuron`

入力は$x_1, \ldots, x_d$です。$d$は、入力レイヤーの*入力数*または*フィーチャの次元*と呼びます。ネットワークの出力は $o_1$ です。単一の数値を予測しようとしているだけなので、出力ニューロンは 1 つだけです。入力値はすべて*指定* であることに注意してください。*計算された*ニューロンは1つだけです。要約すると、線形回帰は、単一層の完全に接続されたニューラルネットワークと考えることができます。今後の章では、はるかに多くの層を持つネットワークに遭遇するでしょう。 

### 生物学

線形回帰は計算神経科学よりも前から存在するため、線形回帰をニューラルネットワークの観点から説明するのは時代錯誤のように思えるかもしれません。それにもかかわらず、サイバネティストと神経生理学者のウォーレン・マカロックとウォルター・ピッツが人工ニューロンのモデルを開発し始めたとき、それらは自然な出発点でした。:numref:`fig_Neuron`の生体ニューロンの漫画的な図を考えてみましょう。*樹状突起*（入力端子）、*核*（CPU）、*軸索*（出力ワイヤ）、および*軸索端子*（出力端子）で構成され、*シナプス*を介して他のニューロンに接続できます。 

![The real neuron.](../img/neuron.svg)
:label:`fig_Neuron`

他のニューロン（または環境センサー）から到着した情報$x_i$は、樹状突起で受信されます。特に、その情報は*シナプスの重み* $w_i$によって重み付けされ、入力の効果、例えば製品$x_i w_i$を介した活性化または阻害を決定する。複数のソースから到着する加重入力は、加重和$y = \sum_i x_i w_i + b$として核に集約され、$\sigma(y)$を介した何らかの非線形後処理の対象となる可能性があります。この情報は、軸索を介して軸索末端に送られ、そこで目的地（筋肉などのアクチュエータなど）に到達するか、樹状突起を介して別のニューロンに供給されます。 

確かに、そのようなユニットの多くを適切な接続性と適切な学習アルゴリズムと組み合わせて、1つのニューロンだけで表現できるよりもはるかに興味深い複雑な動作を生成できるという高レベルのアイデアは、実際の生物学的神経システムの研究のおかげです。同時に、今日のディープラーニングに関するほとんどの研究は、はるかに幅広い情報源からインスピレーションを得ています。私たちはスチュアート・ラッセルとピーター・ノーヴィグ:cite:`Russell.Norvig.2016`を呼び出します。彼らは、飛行機は鳥に*触発された*かもしれないが、鳥類学は何世紀にもわたって航空学の革新の主要な推進力ではなかったと指摘しました。同様に、最近のディープラーニングのインスピレーションは、数学、言語学、心理学、統計、コンピューターサイエンス、および他の多くの分野から同等またはそれ以上の尺度で得られます。 

## まとめ

このセクションでは、従来の線形回帰について紹介しました。この回帰では、学習セットの損失の二乗を最小限に抑えるために線形関数のパラメーターが選択されます。また、いくつかの実際的な考察と、線形性とガウスノイズの仮定の下での最尤推定としての線形回帰の解釈の両方を通じて、この目的の選択を動機付けました。計算上の考慮事項と統計とのつながりの両方について議論した後、そのような線形モデルが、入力が出力に直接接続される単純なニューラルネットワークとしてどのように表現できるかを示しました。間もなく線形モデルを完全に通過する予定ですが、パラメトリック形式、微分可能な目的、ミニバッチ確率的勾配降下法による最適化、そして最終的にはこれまで見られなかったデータの評価など、すべてのモデルが必要とするほとんどのコンポーネントを導入するのに十分です。 

## 演習

1. $x_1, \ldots, x_n \in \mathbb{R}$ というデータがあると仮定します。私たちの目標は、$\sum_i (x_i - b)^2$が最小化されるような定数$b$を見つけることです。
    1. $b$の最適値に対する分析解を見つけます。
    1. この問題とその解決策は正規分布とどのように関係していますか？
    1. 損失を$\sum_i (x_i - b)^2$から$\sum_i |x_i-b|$に変更するとどうなりますか？$b$の最適なソリューションが見つかりますか？
1. $\mathbf{x}^\top \mathbf{w} + b$で表すことができるアフィン関数が、$(\mathbf{x}, 1)$の線形関数と等価であることを証明します。
1. $\mathbf{x}$ の二次関数、つまり $f(\mathbf{x}) = b + \sum_i w_i x_i + \sum_{j \leq i} w_{ij} x_{i} x_{j}$ を求めると仮定します。ディープネットワークでこれをどのように定式化しますか？
1. 線形回帰問題が解ける条件の 1 つは、計画行列 $\mathbf{X}^\top \mathbf{X}$ がフルランクであることを思い出してください。
    1. これが当てはまらない場合はどうなりますか？
    1. どうやって直せる？$\mathbf{X}$ のすべてのエントリに、座標的に独立したガウスノイズを少量加えるとどうなりますか？
    1. この場合の設計行列 $\mathbf{X}^\top \mathbf{X}$ の期待値はどれくらいですか？
    1. $\mathbf{X}^\top \mathbf{X}$がフルランクでない場合、確率的勾配降下法はどうなりますか？
1. 加法性ノイズ $\epsilon$ を支配するノイズモデルが指数分布であると仮定します。つまり、$p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)$ です。
    1. モデル$-\log P(\mathbf y \mid \mathbf X)$のデータの負の対数尤度を書き出します。
    1. クローズドフォームの解決策は見つかりますか？
    1. この問題を解決するミニバッチ確率的勾配降下法アルゴリズムを提案する。何が問題になる可能性がありますか（ヒント：パラメータを更新し続けると、静止点の近くで何が起こるか）？これを直せる？
1. 2つの線形層を構成して、2つの層を持つニューラルネットワークを設計すると仮定します。つまり、最初のレイヤーの出力が 2 番目のレイヤーの入力になります。なぜそのような素朴な構成がうまくいかないのですか？
1. 住宅や株価の現実的な価格見積もりに回帰を使用したい場合はどうなりますか？
    1. 加法性ガウスノイズの仮定が適切でないことを示します。ヒント:マイナス値になることはありますか?ゆらぎはどうですか？
    1. 価格の対数への回帰がはるかに良いのはなぜですか、つまり$y = \log \text{price}$？
    1. ペニーストック、つまり非常に低価格の株を扱う場合、何を心配する必要がありますか？ヒント：可能な限りの価格で取引できますか？なぜこれが安い株にとって大きな問題なのですか？
    1. 詳細については、オプション価格:cite:`Black.Scholes.1973`の有名なBlack-Scholesモデルを参照してください。
1. 回帰を使用して、食料品店で売られているリンゴの*数*を見積もるとします。
    1. ガウス加法性ノイズモデルの問題点は何ですか?ヒント：油ではなくリンゴを売っています。
    1. [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution)は、カウント全体の分布をキャプチャします。$p (k\ mid\ lambda) =\ lambda^k e^ {-\ lambda} /k で与えられます！$. Here $\ ラムダ$ is the rate function and $k$ is the number of events you see. Prove that $\ ラムダ$ is the expected value of counts $k$。
    1. ポアソン分布に関連する損失関数を設計します。
    1. 代わりに $\log \lambda$ を推定する損失関数を設計します。

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/40)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/258)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/259)
:end_tab:
