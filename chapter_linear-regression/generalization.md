# ジェネラライズ
:label:`sec_generalization_basics`

2人の大学生が最終試験に向けて熱心に準備していることを考えてみましょう。一般的に、この準備は、前年に実施された試験を受けることによって、能力を練習し、テストすることで構成されます。それにもかかわらず、過去の試験でうまくやっていても、重要なときに優れているという保証はありません。たとえば、前の年の試験問題の解答を覚えるだけで準備が整っていた学生、エレファンティネ・エリーを想像してみてください。エリーが象の記憶に恵まれていて、*以前に見た*質問に対する答えを完全に思い出すことができたとしても、それでも彼女は新しい（*以前は見られなかった*）質問に直面すると凍りつくかもしれません。比較すると、比較的低い暗記スキルを持つが、パターンを拾うコツがある別の学生、Inductive Ireneを想像してみてください。試験が本当に前年のリサイクルされた質問で構成されている場合、エリーはアイリーンを簡単に上回ります。アイリーンの推測されたパターンが 90% の正確な予測をもたらしたとしても、エリーの100％のリコールと競合することはできませんでした。ただし、試験が完全に新鮮な問題で構成されていたとしても、アイリーンは平均90％を維持する可能性があります。 

機械学習の科学者としての私たちの目標は、*パターン*を発見することです。しかし、単にデータを記憶するのではなく、*一般的な*パターンを本当に発見したことをどうやって確信できるのでしょうか？ほとんどの場合、私たちの予測は、モデルがそのようなパターンを発見した場合にのみ役立ちます。昨日の株価は予測したくないが、明日の株価は予測したい。私たちは、以前に診察された患者についてすでに診断された病気を認識する必要はなく、以前に見られなかった患者の以前に診断されていない病気を認識する必要はありません。この問題、つまり*一般化*するパターンをどのように発見するかは、機械学習の根本的な問題であり、おそらくすべての統計の根本的な問題です。この問題は、科学のすべてを巻き込むはるかに壮大な質問のほんの一部に過ぎないかもしれません。特定の観察からより一般的な声明に飛躍することが正当化されるのはいつですか :cite:`popper2005logic`？ 

実際の生活では、有限なデータコレクションを使用してモデルを適合させる必要があります。そのデータの典型的なスケールは、ドメインによって大きく異なります。多くの重要な医学的問題については、数千のデータポイントにしかアクセスできません。希少疾患を研究するとき、何百もの病気にアクセスできるのは幸運かもしれません。対照的に、ラベル付き写真で構成される最大の公開データセット（ImageNet :cite:`Deng.Dong.Socher.ea.2009` など）には、何百万もの画像が含まれています。また、Flickr YFC100M データセットなど、ラベルのない一部の画像コレクションは、さらに大きくなる可能性があり、1億を超える画像が含まれる :cite:`thomee2016yfcc100m`。しかし、この極端な規模であっても、利用可能なデータポイントの数は、1メガピクセルの解像度で可能なすべての画像のスペースと比較して非常に少ないままです。有限サンプルを扱うときはいつでも、一般化可能なパターンを発見できなかったことを発見するためだけに、トレーニングデータに適合するリスクを念頭に置く必要があります。 

基礎となる分布よりもトレーニングデータに近いフィッティングの現象は*オーバーフィット*と呼ばれ、オーバーフィットに対処するテクニックはしばしば*正則化*メソッドと呼ばれます。統計的学習理論（:citet:`Vapnik98,boucheron2005theory`参照）の適切な導入に代わるものはありませんが、始めるのに十分な直感を提供します。本書全体の多くの章で一般化を再検討し、さまざまなモデルにおける一般化の根底にある原理について知られていることと、実際に関心のあるタスクの一般化を改善するために（経験的に）発見されたヒューリスティック手法の両方を探ります。 

## 学習エラーと汎化エラー

標準の教師あり学習の設定では、トレーニングデータとテストデータは*同一の*分布から*独立*に抽出されると仮定します。これは一般に*IID 仮定*と呼ばれます。この仮定は強力ですが、そのような仮定がなければ、私たちは水中で死んでしまうことに注目する価値があります。なぜ分布$P(X,Y)$からサンプリングされたトレーニングデータが、*異なる分布* $Q(X,Y)$によって生成されたテストデータの予測を行う方法を教えてくれると信じるべきなのでしょうか？このような飛躍を遂げるには、$P$と$Q$がどのように関連しているかについての強い仮定が必要であることが分かります。後で分布の変化を許容するいくつかの仮定について説明しますが、最初に IID ケース、$P(\cdot) = Q(\cdot)$ を理解する必要があります。 

まず、トレーニングデータセットで計算された*統計*である*トレーニングエラー* $R_\text{emp}$と、基礎となる分布に対する*期待値*である*汎化誤差* $R$を区別する必要があります。汎化誤差は、同じ基礎となるデータ分布から抽出された追加のデータ例の無限ストリームにモデルを適用した場合に表示されるものと考えることができます。正式には、トレーニングエラーは*sum*（:numref:`sec_linear_regression`でも同じ表記で）で表されます。 

$$R_\text{emp}[\mathbf{X}, \mathbf{y}, f] = \frac{1}{n} \sum_{i=1}^n l(\mathbf{x}^{(i)}, y^{(i)}, f(\mathbf{x}^{(i)})),$$

一方、汎化誤差は積分として表されます。 

$$R[p, f] = E_{(\mathbf{x}, y) \sim P} [l(\mathbf{x}, y, f(\mathbf{x}))] =
\int \int l(\mathbf{x}, y, f(\mathbf{x})) p(\mathbf{x}, y) \;d\mathbf{x} dy.$$

問題として、汎化誤差 $R$ を正確に計算することはできません。誰も密度関数$p(\mathbf{x}, y)$の正確な形を教えてくれません。さらに、データポイントの無限のストリームをサンプリングすることはできません。したがって、実際には、トレーニングセットから除外された例$\mathbf{X}'$とラベル$\mathbf{y}'$をランダムに選択して構成される独立したテストセットにモデルを適用することにより、汎化誤差を*推定*する必要があります。これは、経験的トレーニングエラーの計算と同じ式を、テストセット $\mathbf{X}', \mathbf{y}'$ に適用することで構成されます。 

重要なのは、テストセットで分類器を評価する場合、*固定*分類器（テストセットのサンプルに依存しない）を使用して作業しているため、その誤差を推定することは単に平均推定の問題です。しかし、トレーニングセットについても同じことは言えません。最終的なモデルは、トレーニングセットの選択に明示的に依存するため、トレーニングエラーは一般に、基礎となる母集団の真のエラーの偏った推定値になることに注意してください。ジェネラライズの中心的な問題は、トレーニングエラーが母集団エラー（つまりジェネラライズエラー）に近いと予想されるのはいつですか。 

### モデルの複雑さ

古典理論では、単純なモデルと豊富なデータがある場合、学習と汎化の誤差は近い傾向があります。しかし、より複雑なモデルやより少ない例で作業する場合、トレーニングエラーは減少しますが、汎化のギャップは大きくなると予想されます。これは驚くべきことではありません。$n$の例のどのデータセットでも、ランダムに割り当てられた場合でも、任意のラベルに完全に適合する一連のパラメーターを見つけることができるほど表現力豊かなモデルクラスを想像してみてください。この場合、トレーニングデータを完全に適合させたとしても、汎化誤差についてどのように結論付けることができますか？私たちが知っているすべてのために、私たちの汎化誤差はランダムな推測に勝るものではないかもしれません。 

一般に、モデルクラスに制限がない限り、トレーニングデータのフィッティングだけでは、モデルが一般化可能なパターン :cite:`vapnik1994measuring` を発見したと結論付けることはできません。一方、モデルクラスが任意のラベルを適合させることができない場合は、パターンを発見したに違いありません。モデルの複雑さに関する学習理論的アイデアは、偽造可能性の基準を形式化した影響力のある科学哲学者であるカール・ポッパーのアイデアからインスピレーションを得ました。ポッパーによると、あらゆる観察を説明できる理論は、まったく科学的な理論ではありません！結局のところ、可能性を排除していなければ、世界について何を教えてくれましたか？要するに、私たちが望むのは、私たちがおそらく行う可能性のある観察を*説明できず、それでも、*実際に*行った観察と互換性があるという仮説です。 

さて、モデルの複雑さの適切な概念を正確に構成するものは複雑な問題です。多くの場合、より多くのパラメータを持つモデルは、任意に割り当てられた多数のラベルに適合できます。しかし、これは必ずしもそうではありません。たとえば、カーネルメソッドは無限の数のパラメータを持つスペースで動作しますが、その複雑さは他の手段 :cite:`scholkopf2002learning` によって制御されます。複雑さの概念として、しばしば有用であることが証明されるのは、パラメータが取ることができる値の範囲です。ここで、パラメータが任意の値を取ることを許可されているモデルは、より複雑になります。このアイデアは、次のセクションで、初めての実用的な正則化手法である*重量減衰*を紹介するときに再考します。特に、実質的に異なるモデルクラス (決定木とニューラルネットワークなど) のメンバー間で複雑さを比較するのは難しい場合があります。 

この時点で、ディープニューラルネットワークを導入する際に再検討するもう1つの重要な点を強調する必要があります。モデルが任意のラベルを近似できる場合、学習誤差が小さいからといって、必ずしも汎化誤差が小さいことを意味するわけではありません。
*ただし、必ずしも
高い汎化誤差も暗示する！* 私たちが自信を持って言えることは、低いトレーニングエラーだけでは低い汎化エラーを証明するのに十分ではないということです。ディープニューラルネットワークは、まさにそのようなモデルであることがわかります。実際にはうまく一般化されていますが、トレーニングエラーだけに基づいて多くの結論を出すには強力すぎます。このような場合、事後に一般化を証明するために、ホールドアウトデータにもっと大きく依存する必要があります。ホールドアウトデータ、つまり検証セットのエラーは、*検証エラー* と呼ばれます。 

## アンダーフィットかオーバーフィッティング？

トレーニングエラーと検証エラーを比較するときは、2 つの一般的な状況に注意する必要があります。まず、トレーニングエラーと検証エラーの両方が大きいが、両者の間に少しギャップがある場合に注意します。モデルがトレーニングエラーを減らすことができない場合は、モデルが単純すぎる（つまり、表現力が不十分な）ため、モデル化しようとしているパターンをキャプチャできない可能性があります。さらに、トレーニングエラーとジェネラライズエラーの間の*汎化ギャップ*（$R_\text{emp} - R$）は小さいので、より複雑なモデルで回避できると信じる理由があります。この現象は*アンダーフィット*として知られています。 

一方、上で説明したように、トレーニングエラーが検証エラーよりも大幅に低く、深刻な*オーバーフィット*を示しているケースに注意する必要があります。オーバーフィットは必ずしも悪いことではないことに注意してください。特にディープラーニングでは、最良の予測モデルが、ホールドアウトデータよりもトレーニングデータの方がはるかに優れたパフォーマンスを発揮することがよくあります。最終的に、私たちは通常、汎化誤差を低くすることを重視し、そのための障害となる限りギャップのみを気にします。学習誤差がゼロの場合、汎化ギャップは汎化誤差と正確に等しくなり、ギャップを減らすことによってのみ進歩できることに注意してください。 

### 多項式曲線フィッティング
:label:`subsec_polynomial-curve-fitting`

過適合とモデルの複雑さに関するいくつかの古典的な直感を説明するために、以下を考えてみましょう。単一の特徴量$x$と対応する実数値のラベル$y$で構成されるトレーニングデータを考えると、次数$d$の多項式を見つけようとします。 

$$\hat{y}= \sum_{i=0}^d x^i w_i$$

$y$というラベルを推定します。これは単なる線形回帰問題であり、私たちの特徴は$x$の累乗によって与えられ、モデルの重みは$w_i$によって与えられ、バイアスはすべての$x$について$x^0 = 1$から$w_0$によって与えられます。これは単なる線形回帰問題なので、二乗誤差を損失関数として使用できます。 

高次の多項式関数は低次の多項式関数よりも複雑です。これは、高次の多項式にはより多くのパラメーターがあり、モデル関数の選択範囲が広いためです。トレーニングデータセットを修正すると、高次多項式関数は、低次多項式に比べて、常に (最悪の場合、等しい) 学習誤差が小さくなるはずです。実際、各データ例が$x$という異なる値を持つ場合は常に、データ例の数と等しい次数を持つ多項式関数は、学習セットに完全に適合できます。:numref:`fig_capacity_vs_error`では、多項式の次数（モデルの複雑さ）とアンダーフィットと過適合の関係を視覚化します。 

![Influence of model complexity on underfitting and overfitting](../img/capacity-vs-error.svg)
:label:`fig_capacity_vs_error`

### データセットのサイズ

上記の境界がすでに示しているように、もう1つ留意すべき大きな考慮事項はデータセットのサイズです。モデルを修正すると、トレーニングデータセットに含まれるサンプルが少なくなるほど、過適合に遭遇する可能性が高くなります（そして深刻になります）。トレーニングデータの量を増やすと、汎化誤差は一般的に減少します。さらに、一般に、より多くのデータが害を及ぼすことはありません。固定タスクとデータ分散の場合、モデルの複雑さはデータ量よりも急速に増加するべきではありません。より多くのデータがあれば、もっと複雑なモデルを近似しようとするかもしれません。十分なデータがないと、単純なモデルは打ち負かすのが難しいかもしれません。多くのタスクにおいて、ディープラーニングは、何千ものトレーニング例が利用可能な場合にのみ、線形モデルよりも優れています。ディープラーニングの現在の成功は、インターネット企業、安価なストレージ、接続されたデバイス、および経済の広範なデジタル化から生まれた大量のデータセットに大きく起因しています。 

## モデル選択
:label:`subsec_generalization-model-selection`

通常、さまざまな方法（異なるアーキテクチャ、トレーニング目標、選択された機能、データの前処理、学習率など）が異なる複数のモデルを評価した後にのみ、最終モデルを選択します。多くのモデルの中から選ぶことを適切に*モデル選択*と呼びます。 

原則として、すべてのハイパーパラメータを選択するまでテストセットに触れないでください。モデル選択プロセスでテストデータを使用する場合、テストデータを過剰適合させるリスクがあります。そうすれば、私たちは深刻なトラブルに陥るでしょう。トレーニングデータを過剰に適合させすぎると、常に正直さを保つためのテストデータの評価があります。しかし、もし私たちがテストデータを過剰適合させたら、どうやってわかるでしょうか？複雑さを厳密に制御できるモデルであっても、これが不合理な結果につながる例については、:citet:`ong2005learning`を参照してください。 

したがって、モデルの選択にテストデータに頼るべきではありません。それでも、モデルをトレーニングするために使用するデータそのものの汎化誤差を推定できないため、モデル選択のためにトレーニングデータだけに頼ることはできません。 

実際のアプリケーションでは、画像が濁ります。最良のモデルを評価したり、少数のモデルを相互に比較したりするために、テストデータに一度だけ触れるのが理想的ですが、実際のテストデータが1回使用されただけで破棄されることはほとんどありません。実験のラウンドごとに新しいテストセットを用意することはほとんどありません。実際、ベンチマークデータを数十年にわたってリサイクルすることは、[image classification](https://paperswithcode.com/sota/image-classification-on-imagenet)や[optical character recognition](https://paperswithcode.com/sota/image-classification-on-mnist)などのアルゴリズムの開発に大きな影響を与える可能性があります。 

*テストセットでのトレーニング*の問題に対処するための一般的な方法は、トレーニングデータセットとテストデータセットに加えて*検証セット*を組み込んで、データを3つの方法で分割することです。その結果、検証データとテストデータの境界が心配なほど曖昧になるという、あいまいな習慣が生まれます。特に明記されていない限り、この本の実験では、真のテストセットなしで、トレーニングデータと検証データと呼ばれるべきものを実際に扱っています。したがって、本の各実験で報告されている精度は、実際には検証精度であり、真のテストセット精度ではありません。 

### クロスバリデーション

トレーニングデータが不足している場合、適切な検証セットを構成するのに十分なデータを保持する余裕さえないかもしれません。この問題の一般的な解決策の 1 つは、$K$*-fold 交差検証* を採用することです。ここでは、元のトレーニングデータが $K$ の重複しないサブセットに分割されます。次に、モデルトレーニングと検証が $K$ 回実行されます。毎回、$K-1$ サブセットでトレーニングを行い、別のサブセット (そのラウンドではトレーニングに使用されなかったサブセット) で検証します。最後に、$K$ の実験の結果を平均化して、学習エラーと検証エラーを推定します。 

## まとめ

このセクションでは、機械学習における汎化の基盤のいくつかを探りました。これらのアイデアのいくつかは、より深いモデルに到達すると複雑で直観に反します。そこでは、モデルはデータをひどく過剰適合させる可能性があり、複雑さの関連する概念は暗黙的で直感に反する可能性があります（たとえば、より多くのパラメータを持つより大きなアーキテクチャがより適切に一般化されます）。いくつかの経験則を残しておきます。 

1. モデル選択には検証セット (または $K$*-fold 交差検証*) を使用します。
1. より複雑なモデルでは、多くの場合より多くのデータが必要です。
1. 関連する複雑さの概念には、パラメータの数と許容される値の範囲の両方が含まれます。
1. 他のすべてを等しく保つと、ほとんどの場合、より多くのデータがより良い一般化につながります。
1. この一般化の話はすべて、IIDの仮定に基づいています。この仮定を緩和して、分布がトレイン期間とテスト期間の間でシフトできるようにすると、さらに（おそらくより穏やかな）仮定がなければ、一般化については何も言えません。

## 演習

1. 多項式回帰の問題を正確に解くことができるのはいつですか？
1. 従属確率変数によって問題を IID データとして扱うことが推奨されない例を少なくとも 5 つ挙げてください。
1. トレーニングエラーがゼロになることは期待できますか？汎化誤差がゼロになるのはどのような状況ですか？
1. $K$倍の交差検証が計算に非常にコストがかかるのはなぜですか?
1. $K$ 分割交差検証誤差推定に偏りがあるのはなぜですか?
1. VCディメンションは、関数クラスの関数によって任意のラベル$\{\pm 1\}$で分類できるポイントの最大数として定義されます。関数のクラスがどれほど複雑であるかを測定するのにこれが良い考えではないのはなぜですか？ヒント:関数の大きさはどうですか?
1. 上司から、現在のアルゴリズムがあまりうまく機能しない難しいデータセットが提供されました。もっとデータが必要だということを彼にどう正当化しますか？ヒント:データを増やすことはできませんが、減らすことはできます。

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/96)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/97)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/234)
:end_tab:
