# ソフトマックスリグレッション
:label:`sec_softmax`

:numref:`sec_linear_regression`では、線形回帰を導入し、:numref:`sec_linear_scratch`でゼロから実装を行い、:numref:`sec_linear_concise`のディープラーニングフレームワークの高レベルAPIを使用して重い作業を行いました。 

回帰は、私たちが答えたいときに手を伸ばすハンマーです*どれくらいですか？* または*いくつですか？* 質問。家が売られる金額（価格）、野球チームの勝利数、または患者が退院するまでに入院する日数を予測したい場合は、おそらく回帰モデルを探しているでしょう。ただし、回帰モデル内でも重要な違いがあります。たとえば、住宅の価格がマイナスになることはなく、変動はベースライン価格に対して*相対的*になることがよくあります。そのため、価格の対数で回帰する方が効果的かもしれません。同様に、患者が入院する日数は*離散非負*の確率変数です。そのため、最小平均二乗法も理想的なアプローチではないかもしれません。この種のイベントまでの時間モデリングには、*サバイバルモデリング*と呼ばれる特殊なサブフィールドで対処される他の多くの複雑さが伴います。 

ここでのポイントは、あなたを圧倒することではなく、単に二乗誤差を最小化するだけではないことを推定することがたくさんあることを知らせることです。そして、もっと広義には、回帰よりも教師あり学習の方がたくさんあります。このセクションでは、*分類*の問題に焦点を当てます。*どれくらいですか？* 質問し、代わりに*どのカテゴリに焦点を当てますか？* 質問。 

* このメールは迷惑メールフォルダまたは受信トレイに属していますか？
* この顧客は、サブスクリプションサービスにサインアップする可能性が高いですか、それともサインアップしない可能性が高くなりますか？
* この画像はロバ、犬、猫、またはオンドリを描いていますか？
* アストンが次に視聴する可能性が最も高い映画はどれですか？
* その本のどのセクションを次に読むつもりですか。

口語的に、機械学習の実践者は、2つの微妙に異なる問題を説明するために*分類*という言葉をオーバーロードします。（i）カテゴリ（クラス）への例のハードアサインのみに関心がある問題、および（ii）ソフトアサインメントを行いたい、つまり各カテゴリーが適用されます。区別が曖昧になる傾向があります。なぜなら、ハードアサインメントだけを考えているときでも、ソフトアサインメントを行うモデルを使用することが多いからです。 

さらに、複数のラベルが当てはまる場合があります。たとえば、ニュース記事では、エンターテインメント、ビジネス、宇宙飛行のトピックを同時に取り上げても、医学やスポーツのトピックは取り上げない場合があります。したがって、それを単独で上記のカテゴリのいずれかに分類することはあまり役に立ちません。この問題は一般に [multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) として知られています。概要については:citet:`Tsoumakas.Katakis.2007`を、画像にタグ付けする際の効果的なアルゴリズムについては:citet:`Huang.Xu.Yu.2015`を参照してください。 

## 分類
:label:`subsec_classification-problem`

足を濡らすために、簡単な画像分類問題から始めましょう。ここで、各入力は $2\times2$ グレースケールイメージで構成されています。各ピクセル値を単一のスカラーで表すことができ、4つの特徴が得られます $x_1, x_2, x_3, x_4$。さらに、各画像が「猫」、「鶏」、「犬」のいずれかのカテゴリに属していると仮定します。 

次に、ラベルの表現方法を選択する必要があります。私たちには2つの明らかな選択肢があります。おそらく最も自然な衝動は、$y \in \{1, 2, 3\}$を選択することでしょう。ここで、整数はそれぞれ$\{\text{dog}, \text{cat}, \text{chicken}\}$を表します。これは、そのような情報をコンピューターに「保存」する素晴らしい方法です。カテゴリに自然な順序付けがある場合、たとえば$\{\text{baby}, \text{toddler}, \text{adolescent}, \text{young adult}, \text{adult}, \text{geriatric}\}$を予測しようとしている場合、これを[ordinal regression](https://en.wikipedia.org/wiki/Ordinal_regression)問題としてキャストし、ラベルをこの形式で保持することも意味があるかもしれません。さまざまなタイプのランキング損失関数の概要については :citet:`Moon.Smola.Chang.ea.2010` を、複数のモードで応答を処理するベイジアンアプローチについては :citet:`Beutel.Murray.Faloutsos.ea.2014` を参照してください。 

一般に、分類問題にはクラス間の自然な順序付けは伴いません。幸いなことに、統計学者ははるか昔にカテゴリデータを表現する簡単な方法、*ワンホットエンコーディング*を発明しました。ワンホットエンコーディングは、カテゴリと同じ数のコンポーネントを持つベクトルです。特定のインスタンスのカテゴリに対応するコンポーネントは 1 に設定され、その他のコンポーネントはすべて 0 に設定されます。この例では、ラベル$y$は3次元ベクトルになり、$(1, 0, 0)$は「猫」、$(0, 1, 0)$は「鶏」、$(0, 0, 1)$は「犬」に対応します。 

$$y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}.$$

### 線形モデル

考えられるすべてのクラスに関連する条件付き確率を推定するには、クラスごとに1つずつ、複数の出力を持つモデルが必要です。線形モデルによる分類に対処するには、出力と同じ数のアフィン関数が必要です。厳密に言えば、最後のカテゴリは$1$と他のカテゴリの合計との差でなければならないため、1つ少なくする必要がありますが、対称性の理由から、わずかに冗長なパラメータ化を使用します。各出力は、独自のアフィン関数に対応しています。この例では、4 つのフィーチャと 3 つの可能な出力カテゴリがあるため、重みを表すには 12 個のスカラー (添字付き $w$) と、バイアスを表す 3 個のスカラー (添字付き $b$) が必要です。これにより、次の結果が得られます。 

$$
\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\
o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\
o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.
\end{aligned}
$$

対応するニューラルネットワーク図は、:numref:`fig_softmaxreg`に示されています。線形回帰と同様に、単層ニューラルネットワークを使用します。また、各出力 $o_1, o_2$ および $o_3$ の計算は、すべての入力、$x_1$、$x_2$、$x_3$、および $x_4$ に依存するため、出力層は*完全結合層* として記述することもできます。 

![Softmax regression is a single-layer neural network.](../img/softmaxreg.svg)
:label:`fig_softmaxreg`

より簡潔な表記法には、ベクトルと行列を使用します。$\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$は、数学とコードにはるかに適しています。すべての重みを$3 \times 4$行列に集め、すべてのバイアスをベクトルに$\mathbf{b} \in \mathbb{R}^3$に集めたことに注意してください。 

### ザ・ソフトマックス
:label:`subsec_softmax_operation`

適切な損失関数を仮定すると、$\mathbf{o}$とラベル$\mathbf{y}$の差を最小化することを直接試みることができます。分類をベクトル値回帰問題として扱うことは驚くほどうまく機能することがわかりますが、それでも以下の点では欠けています。 

* 出力 $o_i$ の合計が $1$ になるという保証はありません。
* 出力の合計が$1$になったり、$1$を超えない場合でも、出力$o_i$が非負になるという保証はありません。

どちらの側面でも、推定の問題を解決するのが難しく、解が外れ値に対して非常に脆弱になります。たとえば、寝室の数と誰かが家を買う可能性の間に正の線形依存があると仮定すると、大邸宅を購入する確率は$1$を超える可能性があります。そのため、出力を「押しつぶす」メカニズムが必要です。 

この目標を達成するには多くの方法があります。たとえば、出力 $\mathbf{o}$ は $\mathbf{y}$ の破損したバージョンであると仮定できます。この場合、破損は、正規分布から引き出されるノイズ $\mathbf{\epsilon}$ を追加することによって発生します。つまり、$\mathbf{y} = \mathbf{o} + \mathbf{\epsilon}$、ここで $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ です。これは、:citet:`Fechner.1860`によって最初に導入された、いわゆる[probit model](https://en.wikipedia.org/wiki/Probit_model)です。魅力的ではありますが、ソフトマックスと比較すると、うまく機能しないか、特に素晴らしい最適化問題につながります。 

この目標を達成する (そして非負性を保証する) 別の方法は、指数関数$P(y = i) \propto \exp o_i$を使用することです。これは、条件付きクラス確率が$o_i$の増加に伴って増加し、単調であり、すべての確率が非負であるという要件を実際に満たしています。次に、これらの値をそれぞれ合計で割ることにより、合計が$1$になるように変換できます。このプロセスを*正規化* と呼びます。これら 2 つのピースを組み合わせると、*softmax* 関数が得られます。 

$$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o}) \quad \text{where}\quad \hat{y}_i = \frac{\exp(o_i)}{\sum_j \exp(o_j)}.$$
:eqlabel:`eq_softmax_y_and_o`

$\mathbf{o}$の最大座標は、$\hat{\mathbf{y}}$に従って最も可能性の高いクラスに対応することに注意してください。さらに、ソフトマックス演算は引数間の順序を保持するため、最も高い確率が割り当てられているクラスを決定するためにソフトマックスを計算する必要はありません。 

$$
\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.
$$

ソフトマックスのアイデアは、物理学:cite:`Gibbs.1902`のアイデアを適応させたギブスにまでさかのぼります。さらに遡る, ボルツマン, 現代の熱力学の父, このトリックを使用して、気体分子のエネルギー状態の分布をモデル化しました.特に、ガス中の分子などの熱力学的アンサンブルにおけるエネルギー状態の有病率は、$\exp(-E/kT)$に比例することを発見しました。ここで、$E$は状態のエネルギー、$T$は温度、$k$はボルツマン定数です。統計学者が統計システムの「温度」を上げたり下げたりすることについて話すとき、彼らはより低いまたはより高いエネルギー状態を優先するために$T$を変えることを指します。ギブスの考えに従うと、エネルギーはエラーに相当します。エネルギーベースのモデル :cite:`Ranzato.Boureau.Chopra.ea.2007` は、ディープラーニングの問題を記述するときにこの観点を使用します。 

### ベクトル化
:label:`subsec_softmax_vectorization`

計算効率を向上させるために、計算をデータのミニバッチでベクトル化します。次元 (入力の数) $d$ をもつ $n$ 個の特徴量のミニバッチ $\mathbf{X} \in \mathbb{R}^{n \times d}$ が与えられていると仮定します。さらに、出力に $q$ のカテゴリがあると仮定します。その後、重みは$\mathbf{W} \in \mathbb{R}^{d \times q}$を満たし、バイアスは$\mathbf{b} \in \mathbb{R}^{1\times q}$を満たします。 

$$ \begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned} $$
:eqlabel:`eq_minibatch_softmax_reg`

これにより、マトリックスマトリックス積$\mathbf{X} \mathbf{W}$への支配的な操作が加速されます。さらに、$\mathbf{X}$の各行はデータ例を表すため、softmax演算自体を*rowwise*で計算できます。$\mathbf{O}$の各行について、すべてのエントリをべき乗し、合計で正規化します。ただし、べき乗や大きな数の対数をとらないように注意する必要があります。これは、数値のオーバーフローまたはアンダーフローを引き起こす可能性があるためです。ディープラーニングフレームワークはこれを自動的に処理します。 

## 損失機能
:label:`subsec_softmax-regression-loss-func`

フィーチャー $\mathbf{x}$ から確率 $\mathbf{\hat{y}}$ へのマッピングができたので、このマッピングの精度を最適化する方法が必要です。最尤推定に頼ります。これは、:numref:`subsec_normal_distribution_and_squared_loss`の平均二乗誤差損失の確率的正当化を提供するときに遭遇したのとまったく同じ概念です。 

### 対数尤度

softmax 関数はベクトル $\hat{\mathbf{y}}$ を与えます。これは、$\hat{y}_1$ = $P(y=\text{cat} \mid \mathbf{x})$ などの入力 $\mathbf{x}$ が与えられた場合、各クラスの（推定された）条件付き確率として解釈できます。以下では、特徴量 $\mathbf{X}$ を持つデータセットでは、ラベル $\mathbf{Y}$ がワンホットエンコーディングラベルベクトルを使用して表されると仮定します。次の特徴を考慮して、実際のクラスがモデルに従ってどの程度ありそうかを確認することで、推定値と現実を比較できます。 

$$
P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).
$$

各ラベルはそれぞれの分布 $P(\mathbf{y}\mid\mathbf{x}^{(i)})$ から独立して描画されると仮定するので、因数分解を使用できます。項の積を最大化するのは扱いにくいので、負の対数を使用して負の対数尤度を最小化するという同等の問題を求めます。 

$$
-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),
$$

ここで、$q$ クラスに対するラベル $\mathbf{y}$ とモデル予測 $\hat{\mathbf{y}}$ の任意のペアについて、損失関数 $l$ は次のようになります。 

$$ l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j. $$
:eqlabel:`eq_l_cross_entropy`

後で説明する理由から、:eqref:`eq_l_cross_entropy`の損失関数は一般に*クロスエントロピー損失*と呼ばれます。$\mathbf{y}$は長さ$q$のワンホットベクトルであるため、そのすべての座標$j$の合計は、1つの項を除くすべての項で消滅します。$\hat{y}$ が確率ベクトルである場合、損失 $l(\mathbf{y}, \hat{\mathbf{y}})$ は $0$ によって下方から制限されることに注意してください。$1$ より大きい単一のエントリはないため、負の対数は $0$; $l(\mathbf{y}, \hat{\mathbf{y}}) = 0$ より小さくすることはできません。*確実性* で実際のラベルを予測する場合のみです。$1$ に向かってソフトマックス出力を取得するには、対応する入力 $o_i$ を無限大（または $j \neq i$ の他のすべての出力 $o_j$ を負の無限大）にする必要があるため、これは重みの有限設定では発生しません。私たちのモデルが$0$の出力確率を割り当てることができるとしても、そのような高い信頼性を割り当てるときに発生するエラーは無限の損失を被ります（$-\log 0 = \infty$）。 

### ソフトマックスとクロスエントロピー損失
:label:`subsec_softmax_and_derivatives`

ソフトマックス関数とそれに対応するクロスエントロピー損失は非常に一般的であるため、それらがどのように計算されるかをもう少しよく理解する価値があります。:eqref:`eq_softmax_y_and_o`を:eqref:`eq_l_cross_entropy`の損失の定義に差し込み、ソフトマックスの定義を使用して次のようになります。 

$$
\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j \\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}
$$

何が起こっているのかをもう少しよく理解するために、ロジット$o_j$に関する微分を考えてみましょう。我々が得る 

$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$

言い換えれば、微分は、ソフトマックス演算で表されるモデルによって割り当てられた確率と、ワンホットラベルベクトルの要素で表される実際に起こったことの差です。この意味で、これは回帰で見たものと非常に似ています。勾配は、観測値$y$と推定$\hat{y}$の差でした。これは偶然ではありません。どの指数家族モデルでも、対数尤度の勾配は正確にこの項によって与えられます。この事実により、実際には勾配の計算が簡単になります。 

ここで、単一の結果だけでなく、結果全体の分布を観察する場合を考えてみましょう。ラベル $\mathbf{y}$ には以前と同じ表現を使用できます。唯一の違いは、$(0, 0, 1)$などのバイナリエントリのみを含むベクトルではなく、$(0.1, 0.2, 0.7)$などの汎用確率ベクトルがあることです。:eqref:`eq_l_cross_entropy`の損失$l$を定義するために以前に使用した数学は、解釈が少し一般的であるというだけで、まだうまく機能します。これは、ラベルを超える分布の損失の期待値です。この損失は*クロスエントロピー損失*と呼ばれ、分類問題で最も一般的に使用される損失の1つです。情報理論の基礎だけを紹介することで、名前をわかりやすく説明できます。簡単に言えば、$\mathbf{y}$が発生すると予測されるものと比較して、$\mathbf{y}$をエンコードするビット数を測定します。以下では、非常に基本的な説明を提供します。情報理論の詳細については、:cite:`Cover.Thomas.1999` または :cite:`mackay2003information` を参照してください。 

## 情報理論の基本
:label:`subsec_info_theory_basics`

ディープラーニングの論文の多くは、直感と情報理論の用語を使用しています。それらを理解するには、いくつかの共通言語が必要です。これはサバイバルガイドです。
*情報理論*は問題を扱う
情報（データとも呼ばれる）のエンコード、デコード、送信、および操作を行います。 

### エントロピー

情報理論の中心的な考え方は、データに含まれる情報の量を定量化することです。これにより、データを圧縮する能力に制限が課せられます。ディストリビューション $P$ では、*エントロピー* は次のように定義されます。 

$$H[P] = \sum_j - P(j) \log P(j).$$
:eqlabel:`eq_softmax_reg_entropy`

情報理論の基本定理の1つは、$P$分布からランダムに抽出されたデータをエンコードするには、それをエンコードするために少なくとも$H[P]$「nats」が必要であると述べています。:cite:`Shannon.1948`。「nat」が何であるか疑問に思うなら、それはビットと同等ですが、ベース2のコードではなくベース$e$のコードを使用する場合です。したがって、1つのNATは$\frac{1}{\log(2)} \approx 1.44$ビットです。 

### 驚き

圧縮が予測とどのような関係があるのか疑問に思われるかもしれません。圧縮したいデータのストリームがあるとします。次のトークンを予測するのが常に簡単であれば、このデータは簡単に圧縮できます。ストリーム内のすべてのトークンが常に同じ値をとる極端な例を考えてみましょう。それはとても退屈なデータストリームです！そして、それは退屈なだけでなく、予測も簡単です。これらは常に同じであるため、ストリームの内容を通信するために情報を送信する必要はありません。予測しやすく、圧縮しやすい。 

しかし、すべての出来事を完全に予測できないなら、時々驚かれるかもしれません。イベントに低い確率を割り当てると、私たちの驚きはより大きくなります。クロード・シャノンは、$j$に（主観的な）確率$P(j)$を割り当てた事象$j$を観察したときの*驚き*を定量化するために$\log \frac{1}{P(j)} = -\log P(j)$に落ち着きました。:eqref:`eq_softmax_reg_entropy`で定義されたエントロピーは、データ生成プロセスに真に一致する正しい確率を割り当てたときに、*予想される驚き*です。 

### クロスエントロピー再考

したがって、エントロピーが真の確率を知っている人が経験する驚きのレベルである場合、クロスエントロピーとは何か疑問に思うかもしれません。$H(P, Q)$と示される* $P$*から* $Q$までのクロスエントロピーは、確率$P$に従って実際に生成されたデータを見て、主観的確率$Q$を持つオブザーバーの予想される驚きです。これは $H(P, Q) \stackrel{\mathrm{def}}{=} \sum_j - P(j) \log Q(j)$ によって与えられます。$P=Q$の場合、可能な限り低いクロスエントロピーが達成されます。この場合、$P$から$Q$へのクロスエントロピーは$H(P, P)= H(P)$です。 

要するに、クロスエントロピー分類の目的は2つの方法で考えることができます。（i）観測データの可能性を最大化すること、および（ii）ラベルを通信するために必要な驚き（したがってビット数）を最小限に抑えることです。 

## まとめと議論

このセクションでは、*離散*出力空間での最適化を可能にする、最初の非自明な損失関数に遭遇しました。その設計の鍵は、離散カテゴリを確率分布から引き出すインスタンスとして扱う確率論的アプローチを取ることでした。副作用として、通常のニューラルネットワーク層の出力を有効な離散確率分布に変換する便利なアクティベーション関数であるsoftmaxに遭遇しました。ソフトマックスと組み合わせた場合のクロスエントロピー損失の微分は、二乗誤差の導関数と非常によく似た動作をすることがわかりました。つまり、予想される動作とその予測の差をとることによるものです。そして、私たちはその表面をスクラッチすることしかできませんでしたが、統計物理学と情報理論との刺激的なつながりに出会いました。 

これはあなたをあなたの道に連れて行くのに十分であり、うまくいけばあなたの食欲を刺激するのに十分ですが、私たちはここで深く潜ることはほとんどありませんでした。とりわけ、計算上の考慮事項をスキップしました。具体的には、$d$入力と$q$出力を備えた全接続層の場合、パラメータ化と計算コストは$\mathcal{O}(dq)$であり、実際には非常に高くなる可能性があります。幸いなことに、$d$入力を$q$出力に変換するこのコストは、近似と圧縮によって削減できます。たとえば、Deep Fried Convnets :cite:`Yang.Moczulski.Denil.ea.2015` は、順列、フーリエ変換、スケーリングの組み合わせを使用して、コストを二次から対数線形に削減します。同様の手法は、より高度な構造マトリックス近似 :cite:`sindhwani2015structured` でも機能します。最後に、圧縮係数$n$に基づく計算コストとストレージコスト:cite:`Zhang.Tay.Zhang.ea.2021`とわずかな精度をトレードオフする意思がある場合は、クォータニオンのような分解を使用してコストを$\mathcal{O}(\frac{dq}{n})$に削減できます。これは活発な研究分野です。難しいのは、必ずしも最もコンパクトな表現や最小数の浮動小数点演算ではなく、最新のGPUで最も効率的に実行できるソリューションを目指すことです。 

## 演習

1. 指数ファミリーとソフトマックスの関係をもう少し詳しく調べることができます。
    1. ソフトマックスのクロスエントロピー損失 $l(\mathbf{y},\hat{\mathbf{y}})$ の 2 次導関数を計算します。
    1. $\mathrm{softmax}(\mathbf{o})$ で与えられる分布の分散を計算し、上記で計算した 2 次導関数と一致することを示します。
1. 等しい確率で発生するクラスが 3 つあると仮定します。つまり、確率ベクトルは $(\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$ です。
    1. バイナリコードを設計しようとすると何が問題になりますか？
    1. もっと良いコードを設計できますか？ヒント:2つの独立した観測値をエンコードしようとするとどうなりますか?$n$の観測値を一緒にエンコードするとどうなるでしょうか？
1. 物理ワイヤを介して送信される信号をエンコードする場合、エンジニアは常にバイナリコードを使用するとは限りません。たとえば、[PAM-3](https://en.wikipedia.org/wiki/Ternary_signal)は、2つのレベル$\{0, 1\}$とは対照的に、3つの信号レベル$\{-1, 0, 1\}$を使用します。$\{0, \ldots, 7\}$の範囲の整数を送信するには、いくつの三元単位が必要ですか？エレクトロニクスの観点から、なぜこれが良いアイデアなのでしょうか？
1. [ブラッドリー・テリーモデル](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model) は
好みを捉えるためのロジスティックモデル。ユーザーがリンゴとオレンジのどちらかを選ぶには、$o_{\mathrm{apple}}$と$o_{\mathrm{orange}}$のスコアを仮定します。私たちの要件は、スコアが大きいほど関連するアイテムを選択する可能性が高くなり、スコアが最も大きいアイテムが最も選択される可能性が高くなることです。:cite:`Bradley.Terry.1952`。
    1. softmax がこの要件を満たしていることを証明します。
    1. リンゴもオレンジも選択しないというデフォルトのオプションを許可したい場合はどうなりますか？ヒント:ユーザーには 3 つの選択肢があります。
1. Softmax の名前は、$\mathrm{RealSoftMax}(a, b) = \log (\exp(a) + \exp(b))$ というマッピングから派生しています。
    1. $\mathrm{RealSoftMax}(a, b) > \mathrm{max}(a, b)$であることを証明してください。
    1. 両方の機能の違いはどれくらい小さくできますか？ヒント:の損失なし
    一般性では、$b = 0$と$a \geq b$を設定できます。
    1. $\lambda > 0$という条件で、これが$\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b)$に当てはまることを証明してください。
    1. $\lambda \to \infty$には$\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) \to \mathrm{max}(a, b)$があることを示します。
    1. ソフトミンはどのようなものですか？
    1. これを3つ以上の数字に拡張します。
1. 関数 $g(\mathbf{x}) \stackrel{\mathrm{def}}{=} \log \sum_i \exp x_i$ は、[ログパーティション関数](https://en.wikipedia.org/wiki/Partition_function_(mathematics) と呼ばれることもあります。
    1. 関数が凸であることを証明します。ヒント:そのためには、一次導関数がソフトマックス関数の確率になるという事実を利用して、2 次導関数が分散であることを示します。
    1. $g$が翻訳不変であること、つまり$g(\mathbf{x} + b) = g(\mathbf{x})$であることを示してください。
    1. $x_i$ の座標の一部が非常に大きい場合はどうなりますか？すべてが非常に小さい場合はどうなりますか？
    1. $b = \mathrm{max}_i x_i$を選択した場合、数値的に安定した実装になることを示してください。
1. いくつかの確率分布 $P$ があると仮定します。$\alpha > 0$ のために $Q(i) \propto P(i)^\alpha$ を持つ別のディストリビューション $Q$ を選択したとします。
    1. $\alpha$のどの選択肢が温度を2倍にするのに対応しますか？それを半分にすることに相当する選択肢はどれですか？
    1. 温度を$0$に収束させるとどうなりますか？
    1. 温度を$\infty$に収束させるとどうなりますか？

[Discussions](https://discuss.d2l.ai/t/46)
