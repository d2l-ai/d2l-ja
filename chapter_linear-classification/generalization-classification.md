# 分類における一般化

:label:`chap_classification_generalization` 

これまで、複数の出力とソフトマックス関数を使用して (線形) ニューラルネットワークをトレーニングすることにより、マルチクラス分類問題に取り組む方法に焦点を当ててきました。モデルの出力を確率的予測として解釈し、クロスエントロピー損失関数を動機付けて導き出しました。クロスエントロピー損失関数は、（固定パラメーターセットに対して）モデルが実際のラベルに割り当てる負の対数尤度を計算します。そして最後に、モデルをトレーニングセットに適合させることで、これらのツールを実践しました。しかし、いつものように、私たちの目標は、これまでに見られなかったデータ（テストセット）で経験的に評価された*一般的なパターン*を学ぶことです。トレーニングセットの精度が高いということは何の意味もありません。各入力が一意である場合（そして実際、これはほとんどの高次元のデータセットに当てはまります）、最初のトレーニングエポックでデータセットを記憶し、新しい画像が表示されるたびにラベルを検索するだけで、トレーニングセットで完全な精度を得ることができます。それでも、正確なトレーニング例に関連付けられた正確なラベルを覚えても、新しい例を分類する方法を教えてくれません。さらなるガイダンスがなければ、新しい例に出会うたびにランダムな推測に頼らなければならないかもしれません。 

多くの燃えるような質問には早急な注意が必要です:
1. 基礎となる母集団の分類器の精度を正確に推定するには、いくつの検定例が必要ですか？
1. 同じテストでモデルを繰り返し評価し続けるとどうなりますか？
1. 線形モデルをトレーニングセットに適合させることが、私たちの素朴な暗記スキームよりもうまくいくと期待すべきなのはなぜですか？

:numref:`sec_generalization_basics`では線形回帰のコンテキストで過適合と汎化の基本を紹介しましたが、この章では統計的学習理論の基本的な考え方をいくつか紹介します。私たちはしばしば汎化を保証できることがわかります*アプリオリ*：多くのモデルと、汎化ギャップ$\epsilon$の希望する上限に対して、必要なサンプル数$n$を決定できることがよくあります。これにより、トレーニングセットに少なくとも$n$のサンプルが含まれている場合、経験的誤差真のエラーの$\epsilon$以内にあるでしょう、
*あらゆるデータ生成ディストリビューション用*。
残念なことに、この種の保証は知的ビルディングブロックの深遠なセットを提供するものの、ディープラーニングの実践者にとって実用的ではないこともわかりました。要するに、これらの保証は、ディープニューラルネットワークを*アプリオリ*一般化するには、不合理な数（おそらく数兆以上）が必要であることを示唆しています。たとえディープニューラルネットワークを気にするタスクで、通常ははるかに少ない例で非常にうまく一般化することがわかったとしても（数千)。したがって、ディープラーニングの実践者は、先験的な保証を完全に放棄することが多く、代わりに過去に同様の問題について十分に一般化してきた方法を使用し、経験的評価を通じて一般化を*事後*ホック*に認定します。:numref:`chap_perceptrons`に到達したら、汎化を再検討し、ディープニューラルネットワークが実際に一般化する理由を説明する試みで生まれた膨大な科学文献への簡単な紹介を提供します。 

## テストセット

汎化誤差を評価するためのゴールドスタンダードの方法としてすでにテストセットに依存し始めているので、そのような誤差推定の特性について議論することから始めましょう。取得方法を気にせずに、固定分類器$f$に焦点を当ててみましょう。さらに、分類器$f$のトレーニングに使用されなかった例$\mathcal{D} = {(\mathbf{x}^{(i)},y^{(i)})}_{i=1}^n$の*新鮮な*データセットを持っているとします。$\mathcal{D}$ の分類器 $f$ の*経験的誤差* は、予測 $f(\mathbf{x}^{(i)})$ が真のラベル $y^{(i)}$ と一致しないインスタンスの割合であり、次の式で与えられます。 

$$\epsilon_\mathcal{D}(f) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(f(\mathbf{x}^{(i)}) \neq y^{(i)}).$$

対照的に、*母集団誤差*は、分類器が真のラベルと一致しない確率密度関数$p(\mathbf{x},y)$によって特徴付けられる、基礎となる母集団（一部の分布$P(X,Y)$）の例の*予想される*割合です。 

$$\epsilon(f) =  E_{(\mathbf{x}, y) \sim P} \mathbf{1}(f(\mathbf{x}) \neq y) =
\int\int \mathbf{1}(f(\mathbf{x}) \neq y) p(\mathbf{x}, y) \;d\mathbf{x} dy.$$

$\epsilon(f)$は私たちが実際に気にする量ですが、一人一人を測定せずに大集団の平均身長を直接観察できないのと同じように、直接観察することはできません。この数量はサンプルに基づいてのみ見積もることができます。この検定セット $\mathcal{D}$ は基礎となる母集団の統計的代表であるため、$\epsilon_\mathcal{D}(f)$ は母集団誤差 $\epsilon(f)$ の統計的推定値と見なすことができます。さらに、対象となる量 $\epsilon(f)$ は（確率変数 $\mathbf{1}(f(X) \neq Y)$ の）期待値であり、対応する推定器 $\epsilon_\mathcal{D}(f)$ はサンプル平均であるため、母集団誤差の推定は単に平均推定の古典的な問題であり、:numref:`sec_prob` から思い出すことができます。 

*中心極限定理*と呼ばれる確率論の重要な古典的結果は、サンプル数$n$が無限大に近づくにつれて、平均$\mu$と標準偏差$\sigma$を持つ任意の分布から抽出された$n$のランダムサンプル$a_1, ..., a_n$を所有するときはいつでも、サンプルが平均$\hat{\mu}$は、真の平均を中心とし、標準偏差$\sigma/\sqrt{n}$の正規分布に近似する傾向があります。すでに、これは重要なことを示しています。例の数が増えるにつれて、テストエラー$\epsilon_\mathcal{D}(f)$は、$\mathcal{O}(1/\sqrt{n})$の割合で真のエラー$\epsilon(f)$に近づくはずです。したがって、テストエラーを 2 倍の精度で推定するには、4 倍の大きさのテストセットを収集する必要があります。テストエラーを100分の1に減らすには、1万倍のテストセットを収集する必要があります。一般に、このような$\mathcal{O}(1/\sqrt{n})$のレートは、統計で期待できる最高のレートです。 

これで、テストエラー $\epsilon_\mathcal{D}(f)$ が真のエラー $\epsilon(f)$ に収束する漸近率について何かわかったので、いくつかの重要な詳細を拡大できます。対象となる確率変数 $\mathbf{1}(f(X) \neq Y)$ は値 $0$ と $1$ しか取ることができず、したがって、値 $1$ を取る確率を示すパラメータによって特徴付けられるベルヌーイ確率変数であることを思い出してください。ここで、$1$は、分類器がエラーを起こしたことを意味するため、確率変数のパラメータは実際には真の誤り率$\epsilon(f)$です。ベルヌーイの分散$\sigma^2$は、式$\epsilon(f)(1-\epsilon(f))$に従ってそのパラメータ（ここでは $\epsilon(f)$）に依存します。$\epsilon(f)$ は当初は不明ですが、$1$ を超えることはできないことがわかっています。この関数を少し調べてみると、真の誤り率が$0.5$に近いときに分散が最も高くなり、$0$に近いか、$1$に近いときにはるかに低くなる可能性があることがわかります。これは、（$n$の試験サンプルの選択に対する）誤差$\epsilon(f)$の推定$\epsilon_\mathcal{D}(f)$の漸近標準偏差が$\sqrt{0.25/n}$より大きくないことを示しています。 

有限のサンプルがあるときではなく、テストセットのサイズが無限大に近づくにつれて、この率が動作を特徴付けるという事実を無視すると、これは、テストエラー$\epsilon_\mathcal{D}(f)$を母集団誤差$\epsilon(f)$に近似させたい場合、1つの標準偏差が次の区間に対応するようにすることを示しています。$\pm 0.01$、それならおよそ2500個のサンプルを集めるべきです。その範囲で2つの標準偏差を適合させ、$\epsilon_\mathcal{D}(f) \in \epsilon(f) \pm 0.01$の95％にする場合、10000サンプルが必要になります。 

これは、機械学習における多くの一般的なベンチマークのテストセットのサイズであることがわかりました。$0.01$以下のエラー率の改善により、毎年何千もの応用ディープラーニングの論文が出版され、大きな成果を上げていることに驚かれるかもしれません。もちろん、エラー率が$0$にかなり近い場合、$0.01$の改善は確かに大きな問題になる可能性があります。 

これまでの分析の厄介な特徴の1つは、実際には漸近的、つまりサンプルサイズが無限大になるにつれて$\epsilon_\mathcal{D}$と$\epsilon$の関係がどのように進化するかについてのみ教えてくれるということです。幸いなことに、確率変数は有界であるため、Hoeffding (1963) による不等式を適用することにより、有効な有限標本境界を得ることができます。 

$$P(\epsilon_\mathcal{D}(f) - \epsilon(f) \geq t) < \exp\left( - 2n t^2 \right).$$

推定値$\epsilon_\mathcal{D}(f)$と真の誤り率$\epsilon(f)$の間の距離$t$が$0.01$を超えないことを95％の信頼度で結論付けることができる最小のデータセットサイズを解くと、$10000$の例と比較して、およそ$15000$の例が必要であることがわかります。上記の漸近的分析によって示唆された。統計を深く掘り下げると、この傾向が一般的に当てはまることがわかります。有限サンプルでも保持される保証は、通常、やや保守的です。物事のスキームでは、これらの数字はそれほど離れていないことに注意してください。これは、私たちが法廷に持ち込むことができる保証ではないとしても、球場の数字を与えるための漸近分析の一般的な有用性を反映しています。 

## テストセットの再利用

ある意味では、経験的な機械学習研究を成功させる準備が整いました。ほぼすべての実用的なモデルは、テストセットの性能に基づいて開発および検証されており、テストセットのマスターになりました。固定分類器 $f$ については、その検定誤差 $\epsilon_\mathcal{D}(f)$ を評価し、母集団誤差 $\epsilon(f)$ について何が言える（またはできないか）を正確に把握していることを知っています。 

それでは、この知識を活用して、最初のモデル $f_1$ をトレーニングする準備をするとします。分類器の誤り率の性能にどの程度自信があるかを知ることで、上記の分析を適用して、テストセット用に取っておくべき適切な数の例を決定します。さらに、:numref:`sec_generalization_basics`の教訓を心に留め、予備分析、ハイパーパラメータ調整、および検証セット上の複数の競合するモデルアーキテクチャの中からの選択をすべて実行することにより、テストセットの神聖さを確実に維持したと仮定します。最後に、モデル $f_1$ を検定セットで評価し、関連する信頼区間で母集団誤差の偏りのない推定値を報告します。 

これまでのところ、すべてが順調に進んでいるようです。しかし、その夜、あなたは午前3時に新しいモデリングアプローチの素晴らしいアイデアで目を覚まします。翌日、新しいモデルをコーディングし、検証セットでハイパーパラメータを調整すると、新しいモデル $f_2$ が動作するようになるだけでなく、エラー率が $f_1$ よりもはるかに低いように見えます。しかし、最終評価の準備をするにつれて、発見のスリルは突然薄れます。テストセットがない！ 

元のテストセット $\mathcal{D}$ がまだサーバー上に存在していても、2 つの手ごわい問題に直面しています。まず、テストセットを収集するときに、単一の分類器 $f$ を評価するという仮定の下で、必要な精度レベルを決定しました。ただし、同じテストセットで複数の分類器 $f_1, ..., f_k$ を評価するビジネスに入る場合は、誤検出の問題を考慮する必要があります。以前は、単一の分類器$f$の$\epsilon_\mathcal{D}(f) \in \epsilon(f) \pm 0.01$を95％確信していたため、誤解を招く結果の確率はわずか5％でした。$k$ 分類器が混在していると、テストセットの性能が誤解を招くような分類器が1つもないことを保証するのは難しい場合があります。20 個の分類器が検討されていると、そのうちの少なくとも 1 つが誤解を招くようなスコアを獲得した可能性を排除する権限がまったくない可能性があります。この問題は、統計学の膨大な文献にもかかわらず、科学研究を悩ませている永続的な問題のままである多重仮説検定に関連しています。 

それだけでは不十分な場合は、その後の評価で得られる結果を信用しない特別な理由があります。テストセットのパフォーマンスの分析は、テストセットとの接触がない状態で分類器が選択されたという仮定に基づいていることを思い出してください。したがって、テストセットは基礎となる母集団からランダムに抽出されたものとして見ることができました。ここでは、複数の機能をテストするだけでなく、$f_1$のテストセットのパフォーマンスを観察した後に、後続の関数$f_2$が選択されました。テストセットからの情報がモデラーに漏れると、厳密な意味で再び真のテストセットになることはありません。この問題は*適応型オーバーフィット*と呼ばれ、最近、学習理論家や統計学者にとって非常に興味深いトピックとして浮上しています。:cite:`dwork2015preserving`。幸いなことに、ホールドアウトセットからすべての情報を漏らす可能性があり、理論上の最悪のシナリオは暗いですが、これらの分析は保守的すぎる可能性があります。実際には、実際のテストセットを作成し、できるだけ頻繁に参照しないようにし、信頼区間を報告するときに複数の仮説検定を考慮し、賭け金が高くデータセットサイズが小さい場合は、より積極的に警戒をダイヤルアップするようにしてください。一連のベンチマークチャレンジを実行する場合、各ラウンドの後に古いテストセットを検証セットに降格できるように、複数のテストセットを維持することが推奨されることがよくあります。 

## 統計的学習理論

一度に、*テストセットは私たちが本当に持っているものすべて*ですが、この事実は奇妙に不満足に思えます。まず、*真のテストセット*を持つことはほとんどありません。データセットを作成しているのが自分でない限り、他の誰かが表向きの「テストセット」で自分の分類子をすでに評価している可能性があります。そして、私たちが最初のディブを取得したときでさえ、私たちはすぐに不満を感じます。私たちの数字を信頼できないというかじるような感覚なしに、その後のモデリングの試みを評価できることを願っています。さらに、真のテストセットでさえ、分類器が実際に母集団に一般化されたかどうかを*事後*伝えることしかできず、一般化すべき*先験的*を期待する理由があるかどうかではありません。 

これらの不安を念頭に置いて、経験的データで訓練されたモデルが目に見えないものに一般化できる/一般化する理由と時期を説明する基本原則を解明することを目的とする機械学習の数学的サブフィールドである*統計的学習理論*の魅力を理解するのに十分な準備が整っているかもしれませんデータ。数十年にわたる統計的学習研究者の主な目的の1つは、モデルクラスのプロパティ、データセット内のサンプル数を関連付ける一般化のギャップを制限することでした。 

学習理論家は、学習セット $\mathcal{S}$ で学習および評価された学習済み分類器 $f_\mathcal{S}$ の*経験的誤差* $\epsilon_\mathcal{S}(f_\mathcal{S})$ と、基になる母集団の同じ分類器の真の誤差 $\epsilon(f_\mathcal{S})$ の差を制限することを目的としています。これは、先ほど取り上げた評価の問題と似ているかもしれませんが、大きな違いがあります。以前は、分類器$f$は修正されており、評価目的でのみデータセットが必要でした。実際、固定分類器は一般化されます。（以前は見えなかった）データセットでのその誤差は、母集団誤差の偏りのない推定値です。しかし、分類器が同じデータセットでトレーニングされ、評価されるとき、私たちは何を言うことができますか？トレーニングエラーがテストエラーに近いと確信できるでしょうか？ 

学習した分類器 $f_\mathcal{S}$ が、事前に指定された関数セット $\mathcal{F}$ の中から選択されなければならないと仮定します。テストセットの議論から、単一の分類器の誤差を推定するのは簡単ですが、分類器のコレクションを検討し始めると状況が悪化することを思い出してください。任意の（固定）分類器の経験誤差が高い確率で真の誤差に近い場合でも、分類器の集合を検討したら、集合内の*1つだけ*分類器がひどく誤って推定された誤差を受け取る可能性について心配する必要があります。心配なのは、コレクション内の1つの分類器だけが誤解を招くほど低い誤差を受け取った場合、それを選択し、それによって母集団誤差を大幅に過小評価する可能性があるということです。さらに、線形モデルであっても、そのパラメータは連続的に評価されるため、通常は無限クラスの関数から選択します ($|\mathcal{F}| = \infty$)。 

この問題に対する野心的な解決策の1つは、均一な収束を証明するための解析ツールを開発することです。つまり、高い確率で、クラス$f\in\mathcal{F}$のすべての分類器の経験的誤り率が、真の誤り率に*同時に*収束するということです。言い換えれば、少なくとも$1-\delta$（一部の小さい $\delta$）では、分類器の誤り率$\epsilon(f)$（クラス$\mathcal{F}$のすべての分類器のうち）が何らかの小さな$\alpha$よりも誤って推定されることはないという理論的原則を求めています。明らかに、すべてのモデルクラス$\mathcal{F}$に対してそのような記述を行うことはできません。常に経験的誤差 $0$ を達成するが、基礎となる母集団に対するランダムな推測を決して上回らない記憶マシンのクラスを思い出してください。 

ある意味、メモライザーのクラスは柔軟性が高すぎる。そのような均一な収束結果は成り立たないでしょう。一方、固定分類器は役に立ちません。完全に一般化されますが、学習データにもテストデータにも適合しません。したがって、学習の中心的な問題は、歴史的に、トレーニングデータによりよく適合するが過剰適合のリスクがある、より柔軟な（より高い分散）モデルクラスと、一般化は良好であるが適合不足のリスクがあるより厳格な（より高いバイアス）モデルクラスとの間のトレードオフとして組み立てられてきました。学習理論における中心的な問題は、モデルがこのスペクトルに沿って位置する場所を定量化し、関連する保証を提供するための適切な数学的分析を開発することでした。 

一連の独創的な論文で、VapnikとChervonenkisは、相対周波数の収束に関する理論をより一般的な関数のクラスに拡張しました:cite:`VapChe64,VapChe68,VapChe71,VapChe74b,VapChe81,VapChe91`。この一連の作業の主な貢献の1つは、モデルクラスの複雑さ（柔軟性）を測定する（1つの概念）Vapnik-Chervonenkis（VC）次元です。さらに、それらの主要な結果の1つは、経験誤差と母集団誤差の差を、VC次元とサンプル数の関数として制限します。 

$$P\left(R[p, f] - R_\mathrm{emp}[\mathbf{X}, \mathbf{Y}, f] < \alpha\right) \geq 1-\delta
\ \text{ for }\ \alpha \geq c \sqrt{(\mathrm{VC} - \log \delta)/n}.$$

ここで、$\delta > 0$ は範囲に違反する確率、$\alpha$ は汎化ギャップの上限、$n$ はデータセットのサイズです。最後に、$c > 0$は、発生する可能性のある損失の規模にのみ依存する定数です。この範囲の 1 つの用途は、$\delta$ と $\alpha$ の希望する値を差し込んで、収集するサンプルの数を決定することです。VCディメンションは、任意の（バイナリ）ラベルを割り当てることができるデータポイントの最大数を定量化し、それぞれについて、そのラベルに一致するクラス内のモデル$f$を見つけます。たとえば、$d$ 次元の入力の線形モデルには、VC ディメンション $d+1$ があります。ラインが 2 次元の 3 つのポイントに割り当てることができるが、4 つのポイントには割り当てられないことが簡単にわかります。残念ながら、この理論はより複雑なモデルでは過度に悲観的になる傾向があり、この保証を得るには、通常、目的のエラー率を達成するために実際に必要とされるよりもはるかに多くの例が必要です。また、モデルクラスと$\delta$を修正すると、エラーレートが通常の$\mathcal{O}(1/\sqrt{n})$レートで再び減衰することにも注意してください。$n$に関してもっとうまくやれるとは思えない。しかし、モデルクラスを変えると、VC次元は汎化ギャップの悲観的な描写を示すことができます。 

## まとめ

モデルを評価する最も簡単な方法は、これまで見られなかったデータで構成されるテストセットを調べることです。テストセット評価は、真の誤差の偏りのない推定値を提供し、テストセットが大きくなるにつれて目的の $\mathcal{O}(1/\sqrt{n})$ レートで収束します。正確な漸近分布に基づく近似信頼区間、または（より保守的な）有限サンプル保証に基づく有効な有限サンプル信頼区間を提供できます。実際、テストセットの評価は、現代の機械学習研究の基盤です。ただし、テストセットが真のテストセットになることはほとんどありません（複数の研究者が何度も使用しています）。同じテストセットを使用して複数のモデルを評価すると、誤検出の制御が困難になる可能性があります。これは理論上大きな問題を引き起こす可能性があります。実際には、問題の重要性は、問題のホールドアウト集合のサイズと、それらが単にハイパーパラメータの選択に使用されているのか、それとも情報がより直接的に漏洩しているのかによって異なります。それでも、実際のテストセット（または複数）をキュレートし、それらの使用頻度についてできるだけ控えめにするのが良い習慣です。 

統計的学習理論家は、より満足のいくソリューションを提供するために、モデルクラス全体で一様な収束を保証する方法を開発しました。実際にすべてのモデルの経験誤差が真の誤差に同時に収束する場合、ホールドアウトデータでも同様に良好に機能することがわかっているので、学習誤差を最小限に抑えて、最良のパフォーマンスを発揮するモデルを自由に選択できます。重要なのは、そのような結果はいずれもモデルクラスの何らかのプロパティに依存しなければならないということです。ウラジミール・ヴァプニクとアレクセイ・チェルノヴェンキスはVC次元を導入し、VCクラスのすべてのモデルに当てはまる均一な収束結果を提示しました。クラス内のすべてのモデルのトレーニングエラーは、（同時に）真のエラーに近いことが保証され、$\mathcal{O}(1/\sqrt{n})$のレートでより近くなることが保証されています。VC次元の革新的な発見に続いて、それぞれが類似した一般化保証を容易にする多数の代替複雑度測定が提案されています。関数の複雑度を測定するいくつかの高度な方法の詳細については、:citet:`boucheron2005theory`を参照してください。残念なことに、これらの複雑さの測定は統計理論において広く有用なツールになりましたが、ディープニューラルネットワークが一般化する理由を説明するために（簡単に適用できるように）無力であることが判明しました。ディープニューラルネットワークは多くの場合、数百万のパラメーター (またはそれ以上) を持ち、大量のポイントにランダムなラベルを簡単に割り当てることができます。それにもかかわらず、それらは実際的な問題についてうまく一般化しており、驚くべきことに、より大きなVC寸法を被るにもかかわらず、より大きく、より深くなると、よりよく一般化することがよくあります。次の章では、ディープラーニングの文脈における汎化を再考します。 

## 演習

1. 固定モデル$f$の誤差を、99.9％を超える確率で$0.0001$以内に推定する場合、いくつのサンプルが必要ですか？
1. 他の誰かがラベル付きテストセット $\mathcal{D}$ を所有していて、ラベルのない入力 (フィーチャ) のみを使用可能にするとします。ここで、ラベルのない各入力に対してモデル $f$ (モデルクラスに制限なし) を実行し、対応するエラー $\epsilon_\mathcal{D}(f)$ を受け取ることによってのみテストセットのラベルにアクセスできるとします。実際のエラーに関係なく、テストセット全体をリークする前に、いくつのモデルを評価する必要がありますか。したがって、エラー $0$ があるように見えますか？
1. $5^\mathrm{th}$次多項式のクラスのVC次元は何ですか？
1. 二次元データの軸に整列した長方形のVC次元は何ですか?

[Discussions](https://discuss.d2l.ai/t/6829)
